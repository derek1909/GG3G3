{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad6a89a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IIA project GG3: Neural Data Analysis\n",
    "\n",
    "Easter 2023<br>\n",
    "Project Leader: Yashar Ahmadian (ya311)<br>\n",
    "Group members: Arihant Pandey (ap2207), Junhyuck Kim (jk808), Derek Jinyu Dong (jd976)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad546cd7",
   "metadata": {},
   "source": [
    "## Important dates\n",
    "\n",
    "Project start: __Thursday May 11 2023 9:30am GMT+1 (UK summer time)__ \n",
    "\n",
    "Interim report deadline: ðŸ”¥__Friday 19 May 2023, 4pm__ðŸ”¥ (electronic submission via Moodle)\n",
    "<br>\n",
    "(Interim report should contain report on tasks under the header \"**Week 1**\".\n",
    "\n",
    "Presentations: __Monday 5 June 2023, 11am-12:30pm in LT6__\n",
    "\n",
    "Final project report deadline: __Friday 9 June 2023,  4pm__ (electronic submission via Moodle)\n",
    "\n",
    "\n",
    "## Project notes\n",
    "\n",
    "- You should spend about 20 hours a week on the project, basically half of your time.\n",
    "- Project is to be carried out in **Google Colab** or on your own computer. You can download this notebook and use it with a normal Jupyter server, or duplicate it here in your **Colab** account. If you do the latter, you can share and show your work easily. The computational resources on **Colab** are limited, so you may find it more convenient to run the programs on your own computer, especially in the later parts of the project when computations will be heavier. When you need to ask a question about a specific piece of code, you can still use the **Colab** to share a notebook. \n",
    "- Weekly sessions will be held on Mondays 11:00-13:00, and Thursdays 9:00-11:00 and 14:00-16:00 all in **LR11** (EXCEPT for Monday 5th June which will be in LT6). \n",
    "- Attendance is compulsory for the first/introductory session and all Monday sessions. Thursday sessions will be optional (although this is still subject to change), but attending them is a good way to get answers to questions, some help with coding. It also provides space for teamwork with your teammates. \n",
    "- You are strongly encouraged to seek verbal feedback after your interim report - there will be a special session for this on Monday May 22 and Thursday May 25. \n",
    "- Project carries 80 marks overall:\n",
    "  - 20 marks for interim report (individual)\n",
    "  - 20 marks for presentation (group based)\n",
    "  - 40 marks for final report (individual)<br>\n",
    "  \n",
    "  \n",
    "### Project reports\n",
    "  - Should be clearly broken down by _Tasks_ (see below), any notes you wish to make in how you or your group structured and carried out the tasks, and most importantly your __results__ in the form of completely labelled graphs, and __accompanying conclusions__ you draw from your results. \n",
    "  - Interim report about 4-6 pages, and final report about 14-18 pages, when converted to a PDF (excluding appendices such as attached code, but _including_ figures). The final report can be an extension of the interim report, but make sure you take into account the feedback you receive for your interim report.  \n",
    "  - When deciding what to include in your report, how to organise it and what to emphasize, please prioritise communicating understanding over formalities - I would like give you marks for doing the right thing and showing that you did it and understand it. If I have to wade through pages of undigested data and graphs shown just because it was there, I will feel less generous. The length requirements are only guidelines. \n",
    "  - Take a look at [this page](http://teaching.eng.cam.ac.uk/node/444/#hdr-9) and [this](http://teaching.eng.cam.ac.uk/node/340) for further guidance and recommendations for writing reports.\n",
    "  - __All code__ that you used during to project must be attached as an appendix to your reports. If you modified one of the provided `.py` file (and you used that modified version for that report), include it. \n",
    "  - A jupyter or **Colab** notebook are acceptable as a report, as long as it is \"clean\" (its main section includes text and figures) and reads like a report, and (importantly) can be converted to a PDF, so you can upload it to the Moodle submission protal. \n",
    "  - Incude [cover sheets](http://teaching.eng.cam.ac.uk/node/4171) provided by the Teaching Office\n",
    "  \n",
    "### Presentations:\n",
    "- Each group will jointly prepare and present a 12 minute (strict!) presentation, broken up into three **4-minute parts** each delivered by one of the team members. There will be 3 minutes of question time after each talk, and so overall the session should take about an hour and a half. \n",
    "\n",
    "- Since most tasks are not really divided, the part presented by a student need not be something they solely contributed to. \n",
    "- The presentations will be held at the end of Week 3 or beginning of Week 4.\n",
    "- I will give guidelines and recommendations for making good presentations in due course. \n",
    "\n",
    "## Timeline\n",
    "\n",
    "See the Approximate Timeline section below.\n",
    "\n",
    "## Survey\n",
    "\n",
    "The **online survey** should be completed at the end of the project period. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12f4b8e2-1bbf-4ef0-8e61-356e6533a848",
   "metadata": {},
   "source": [
    "# Neuroscience Background"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa168e21-72a4-43ca-9193-b06b23daf9ec",
   "metadata": {},
   "source": [
    "The background is also provided as a Jupyter notebook [accessible here](https://github.com/ahmadianlab/gg3_nda/blob/main/Background.ipynb). \n",
    "\n",
    "**Note:** The main point of the Background handout is to introduce some terminology (which appear all in boldface),<br>\n",
    "and mathematical notation that will be used in the next section, \"What is the right model of LIP?\"<br> \n",
    "Deep understanding of this Background  section is not required for carrying out the project. But, apart from the<br>\n",
    "above reason, you are encourged to read it to understand the scientific motivations and significance of this problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27ca6c12-5c30-4cc2-bbb1-29fb05547262",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What is the right model of LIP?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d0a6cd-7d16-43f4-9549-cb99481b2d01",
   "metadata": {},
   "source": [
    "### Alternative hypothesis: stepping model\n",
    "\n",
    "As we saw in the Background section, classic studies suggested that LIP neurons which exhibit ramping activity in<br>\n",
    "their trial-averaged PSTH's are involved in evidence accumulation. However, the story became more complicated,<br>\n",
    "when in 2015, [Latimer et al.](https://www.science.org/doi/10.1126/science.aaa4056) provided evidence that most LIP neurons are better modelled<br>\n",
    " as neurons with a \"stepping firing rate\". In this alternative model\n",
    " the rate does not continuously ramp up or down<br>  (albeit via a random walk) as in a drif-diffusion model.\n",
    "Rather, the rate is piece-wise constant:<br> it starts relatively low, but at some time point it jumps (\"steps\") up discontinuously to <br>\n",
    "a higher firing rate level. The jump point is random and varies from trial to trial, according to some distribution. <br>\n",
    "\n",
    "#### -------------------------------   Figure 5   -------------------------------\n",
    "<img src=\"figs/latimer-step-ramp.png\" width=600 height=600 />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ff6abb3-eeb2-4630-8dfb-5ae866a8b81b",
   "metadata": {},
   "source": [
    "We will refer to these two competing hypotheses or models as the **ramping** and **stepping models**, respectively<br>\n",
    "(other common synonyms for the ramping model are \"the drif-diffusion model\", mentioned above, and \"the diffusion-to-bound model\";<br>\n",
    "we will also use **jump model** as synonymous with the stepping model.) \n",
    "\n",
    "In this project we aim to develop tools that allow us to reject or accept one of these hypotheses<br>\n",
    "based on observed spike trains. Understanding which of the two is a more accurate description of LIP activity <br>\n",
    "is scientifically significant. The ramping hypothesis suggests that LIP cortex is responsible<br>\n",
    "for accumulating evidence to inform and make decisions. On the other hand, the binary nature of the stepping model<br>\n",
    "suggests that LIP is downstream of the evidence accumulating area, and may simply reflect, in its activity, the decision already made<br>\n",
    "in an upstream area."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3d78609-a056-4ed8-a703-7baf368e61e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Two generative models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "807d3cc0-3549-4c0f-a568-cb20b12ef24b",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "So far, our two \"models\" have mostly remained conceptual and qualitative. At this high, conceptual level <br>\n",
    "I will therefore refer to them as hypotheses instead: the ramping hypothesis vs. stepping hypothesis. <br>\n",
    "However, in order to use the powerful tools of probability theory and machine learning, we need to <br>\n",
    "turn these conceptual hypotheses into well-defined mathematical models.\n",
    "\n",
    "### The common, abstract model structure\n",
    "The ramping and stepping models to be described here and simulated in the project,  are examples of<br>\n",
    "**probabilistic generative models**. Each model has a set of **parameters** (such as the drift rate of the ramping model),<br>\n",
    "and it stochastically generates data, in our case spike trains. The systematic behaviour of these spike trains depends<br>\n",
    " on the various model parameters. Mathematically, this stochastic relationship between the parameters and data<br>\n",
    " is given by a conditional probability distribution\n",
    "\n",
    "$P(\\mathrm{data}| \\Theta, M)$\n",
    "\n",
    "where $\\Theta$ denotes the set of parameters and $M$ denotes the model (in our case $M$ = ramping, or $M =$ stepping).<br>\n",
    "This conditional probability, when viewed as a function of $\\Theta$, is called the model's **likelihood function**.<br>\n",
    "By (observed) \"data\" we mean a set of spike-trains recorded (in our case simulated) over many trials:\n",
    "\n",
    "$\n",
    "\\text{data} \\equiv \\{(n_t)_{t=1}^T\\}.\n",
    "$\n",
    "\n",
    "$n_t$ will sometimes be referred to as **observed variables**.\n",
    "\n",
    "**Latent variables:** As generative models, the two models can also be simulated to generate spike trains. In order to do this, <br>\n",
    "the two models first generate a firing rate function or time-series, $r_t$. The spike count, $n_t$, in a given<br>\n",
    "time bin is then stochstically generated based solely on $r_t$. The rate sequence $r_t$ is itself a stochastic<br>\n",
    "process, and depends on a set of *latent variables*. Latent variables are random variables that are so called<br>\n",
    "because they are not directly observed by us (data-)scientists and engineers, but need to be inferred from observed<br>\n",
    "data (the spike trains). In the simple version of the stepping model, with which we will start, there is only <br>\n",
    "a single latent variable: the stepping time. The ramping model, on the other hand, generates a whole sequence of <br>\n",
    "latent variables in each trial: these are the values of the ramping stochastic process, which is closely tied to the<br>\n",
    "firing rate.\n",
    "\n",
    "Since the latent variables (unlike the model parameters) vary from trial to trial, in each trial they need to be<br>\n",
    "inferred from a single spike-train. By contrast, parameters which control the systematic behaviour of the model will<br>\n",
    "be inferred from the entire dataset, i.e. the collection of spike trains in all trials.\n",
    "\n",
    "**Discrete vs continuous time:** Both models are implemented in discrete time. Thus the varible $t$ above is an integer<br>\n",
    "(index for the) time-step. We will denote the (fixed) total number of time steps in a trial by $T$. Real trials<br>\n",
    "last on the order of 1 second, and we would want our time steps or time bins to be around 1 to 10 milliseconds. <br>\n",
    "So, correspondingly, $T$ will be rather large, we will experiment with $T=$ 100 to 1000. For various purposes, <br>\n",
    "we will need to convert from discrete to continuous time in seconds. For that purpose we will fix the trial duration at<br>\n",
    "1 second and thus interpret each time-step to have duration $1/T$ seconds; we will denote this by $dt$ here and in the code<br>\n",
    "(thus $dt = 1/T$ seconds).\n",
    "\n",
    "We will now describe the probabilistic structure of the two models in some detail.\n",
    "\n",
    "### Stepping model\n",
    "\n",
    "**Latent variables:** This is the simpler one of the two. The only latent variable of this model is the step time or **jump time**. <br> \n",
    "I will denote the step time in trial $j$ by $\\tau_j$. Since we work in discrete time, $\\tau_j$ is a (non-negative) integer.<br>\n",
    "In `models.py` the corresponding variable is called `jump` or (when containing the value of multiple trials) `jumps`.<br>\n",
    "In each trial, the step timeÂ is sampled from some probability distribtion:\n",
    "\n",
    "$\\tau \\sim P(\\tau)$\n",
    "\n",
    "In the provided code this distribution is a so-called **negative binomial distribution** (see [this](https://en.wikipedia.org/wiki/Negative_binomial_distribution)) with two parameters: $m$ and $r$.<br>\n",
    "$m$ sets the average step time, and $r$ ... that's left for you to figure out.<br>\n",
    "\n",
    "<br>$\\quad$ **negative binomial distribution:**<br>\n",
    "$\\quad$ In each trial the probability of success is p and of failure is 1-p. We observe this sequence until a predefined number r of successes occurs.\n",
    "$ \\qquad \\tau \\sim NB(r,p)$<br>\n",
    "$ \\qquad E[\\tau] = m = r(1-p)/p$<br>\n",
    "\n",
    "As we said above, in each trial, the firing rate sequence of this model is piece-wise constant. If we denote the jump time of trial<br>\n",
    "$j$ by $\\tau_j$, then for $t < \\tau_j$, $r_t = R_0$ and for $t \\geq \\tau_j$, $r_t = R_h > R_0$, where the two constants $R_0$ and $R_h$ are <br>\n",
    "part of the model parameters. We will refer to them as pre- and post-step firing rates. \n",
    "\n",
    "Finally, given the rate sequence, $r_t$, the spike counts in different timesteps are generated indpendently from a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution):\n",
    "\n",
    "$n_t \\sim \\mathrm{Poiss}(r_t dt)$\n",
    "\n",
    "Note  that since we measure rates in Hz, $dt$ has to be in seconds, in order to get the right dimensionless parameter (mean spike count) of <br>\n",
    "of the Poisson distribution. \n",
    "\n",
    "**Fit parameters:** $m, r, \\text{ and } x_0$.\n",
    "\n",
    "These are the parameters which you aim to infer from spike train datasets.<br>\n",
    "$x_0$ is equivalent to $R_0$ and is given by $R_0/R_h$; think of it as the noramalised pre-step rate<br>\n",
    "(we use $x_0$ instead of $R_0$, to match the similar parameter in the ramping model). By definition $0< x_0 < 1$.\n",
    "\n",
    "**\"Fixed\" parameters:** We will take $R_h$ as known/fixed, and will not infer it from data. For many project tasks we will fix it at $R_h = 50$ Hz.<br>\n",
    "Though in early tasks you will explore its effects by varying it.\n",
    "\n",
    "### Ramping model\n",
    "\n",
    "**Latent variables:** This model, which approxmiates the continuous time drift-diffusion model, has a whole sequence of latent variables<br>\n",
    "which we will denote by $x_t$. This variable is the so-called **decision variable**. The update equations for $x_t$ are discretised versions<br>\n",
    "of the equation in Figure 3:\n",
    "\n",
    "$x_{t+1} = x_t + \\beta dt + \\sigma \\sqrt{dt} \\epsilon_t \\qquad\\qquad\\qquad$            Eq. (1)\n",
    "\n",
    "where \n",
    "\n",
    "$\\epsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0,1) \\qquad\\qquad\\qquad$            Eq. (2)\n",
    "\n",
    "(i.e. $\\epsilon_t$ is sampled independently in each time step from the standard normal distribution, $\\mathcal{N}(0,1)$, in other words, it has<br>\n",
    "a Gaussian distribution with mean 0 and variance 1.) The initial condition is set via\n",
    "\n",
    "$\n",
    "x_1 = x_0 + \\sigma \\sqrt{dt} \\epsilon_0\n",
    "$\n",
    "\n",
    "where $\\epsilon_0$ is again standard normal, and $x_0$ is a model parameter (and not a latent variable, <br>\n",
    "as it is the same across all trials). (Note that, due to python indices starting from 0, the equation above will (implicitly)<br>\n",
    "appear as `x[0] = x0 + sigma * np.random.randn()` in the code).\n",
    "\n",
    "The firing rate in this model is a rectified and scaled version of $x_t$:\n",
    "\n",
    "$r_t = R_h [x_t]_+ = R_h \\max(0, x_t)$\n",
    "\n",
    "It is not hard to see that the sequential variables $x_t$ form a Markov chain (this has to do with the fact that $\\epsilon_t$<br> in different\n",
    "trials are independent), and therefore the ramping model is an example of a **hidden Markov model (HMM)**.<br>\n",
    "In fact, if you have taken 3F8, you will realise that $x_t$ is *almost* an AR(1) Gaussian Process. I said almost an AR(1) Gaussian process, because<br>\n",
    "\n",
    "*when $x_t$ reaches 1, it will get stuck there for the rest of the trial*. (Equivalently, after this point, the firing rate, $r_t$, stays at its<br>\n",
    "maximal level $R_h$.)\n",
    "\n",
    "This reflects the interpretation of $x_t$ as a decision variable, which upon reaching a pre-set bound or threshold, triggers<br>\n",
    "the decision; in our case the bound is 1. \n",
    "\n",
    "Similar to the stepping model, given the rate sequence, $r_t$, the spike counts in different timesteps are generated indpendently from a Poisson distribution\n",
    "\n",
    "$n_t \\sim \\mathrm{Poiss}(r_t dt)$.\n",
    "\n",
    "**Fit parameters:** $\\beta, \\sigma, \\text{ and } x_0$.\n",
    "\n",
    "$\\beta$ and $\\sigma$ control the systematic drift vs. stochasticity of the ramping variable $x_t$.<br>\n",
    "Similar to the stepping model, $x_0$ sets the initial rate, $r_0$, via $r_0 = R_h x_0$. And again $0< x_0 < 1$.\n",
    "\n",
    "**\"Fixed\" parameters:**  $R_h$, maximal rate, to be treated as in the stepping time.\n",
    "\n",
    "\n",
    "### Ignored stimulus dependence\n",
    "\n",
    "In the full version of the ramping model, the magnitude and sign of $\\beta$ depends on the coherence and the direction of motion of the RDM stimulus<br>\n",
    "in that trial. However, for simplicity, in this project we assume $\\beta$ is fixed in all trials and assume it is positive.<br>\n",
    "\n",
    "Similarly, in the full version of the stepping model, the post-jump rate can take two possible values $R_h > R_0$, as described above, <br>\n",
    "or $R_l < R_0$. We can call $R_h$ and $R_l$ the up or down rates, and call their normalized values of 1 and $R_l/R_h < 1$ up and down states.<br>\n",
    "The probability with with the model transition up or down after the jump time can again depend on the coherence and direction of motion of the <br>\n",
    "RDM stimulus in a trial. But again, to simplify the model, we ignore this fact. In fact, for most of the project we work with a stepping model<br>\n",
    "without a down state. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbe9c313-952c-4beb-8c39-0100c23d5768",
   "metadata": {},
   "source": [
    "# Approximate timeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57be6ffc-eaf1-4d02-97cd-cd8a8d8d1f79",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our task in this project is to slowly build up techniques to ultimately reject or accept one or the other hypothesis based on \n",
    "recorded (or, in our case, simulated) spike trains. We will do this by progressively moving from lower to higher levels of \n",
    "probabilistic inference. \n",
    "\n",
    "\n",
    "- (week 1) explore the behaviour of the two models based on simulator code provided to you in `models.py`.<br>\n",
    "And take preliminary steps towards developing a discrete-state HMM approximation to them, which allows us to apply  powerful inference tools. \n",
    "\n",
    "- (week 2) develop tools to carry out **single-trial inference**  of the models' latent variables from observed  \n",
    "on single spike trains, taking advantage of their HMM formulation.\n",
    "\n",
    "- (weeks 2-3) Assuming model $M$ is the true model underlying data, use Bayesian or maximum-likelihood inference to infer or estimate<br>\n",
    "model parameters, $\\Theta$, based on observed data, that is, many trials of simulated spike trains. \n",
    "\n",
    "- (weeks 3-4) Use Bayesian inference to select/reject one or the other hypothesis/model, given a dataset of spike trains.<br> \n",
    "\n",
    "Depending on feedback and pace of progress, in week 4 we will also investigate the consequences of model mismatch. Since \"all models are wrong (but some are useful)\",<br> \n",
    "what can we say about the possibility of reaching wrong conclusions regarding our alternative conceptual hypotheses, due to <br>\n",
    "some arbitrary choices we had to make in translating those conceptual models to concrete mathematical models?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c33dd59-3fc8-4a7f-b6bf-7fdf540300d5",
   "metadata": {},
   "source": [
    "# Running on Colab vs Deepnote\n",
    "As I said in the Intro lecture, the project Jupyter notebook is accessible, via these links, on both [Deepnote] and \n",
    "on [Google Colab]. You can choose to run your notebook on either of those (for Deepnote you will need to sign up\n",
    "and create an account with them), or you can download it and run things on your own machine, which may be faster. \n",
    "\n",
    "If you choose to work on Deepnote you will need to \"Duplicate\" the notebook (using the blue\n",
    "button on the top right) so that you can make and save your changes. Similarly, if you use Colab, you will have to\n",
    "\"Save a copy in Drive\" in order to be able to save your changes (if you don't have Google Drive, then either sign up, or download and\n",
    "work on your laptop, or use Deepnote).\n",
    "\n",
    "Finally, the `.py` modules (including Week 1's `models.py`) are accessible and can be download from the Deepnote. \n",
    "On Colab, you will have to run the following cell to import them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa7c4a2-98f7-4f6d-8841-b74dc713750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"local\" # change this to \"local\" if you are on Deepnote or your own computer\n",
    "\n",
    "if mode == \"local\":\n",
    "    import models\n",
    "elif mode == \"colab\":\n",
    "    import requests\n",
    "    url = 'https://github.com/ahmadianlab/gg3_nda/blob/main/models.py?raw=true'\n",
    "    r = requests.get(url)\n",
    "    with open('models.py', 'w') as f:\n",
    "        f.write(r.text)\n",
    "    import models\n",
    "else:\n",
    "    raise Exception(\"mode must be either local or colab\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe0d9133-5bd6-4f34-a680-0268aecef4d6",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df778a9b-3b9c-4330-b5fa-53a9945c2051",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "\n",
    "Study the code in `models.py`, specifically the implementations of the two models in the `StepModel` and `RampModel` classes. <br>\n",
    "The main part to study (and relate to the mathematical discussion above) is their `simulate` method/function. You create <br>\n",
    "an object instance of each model by providing the model parameters (both \"fit\" and \"fixed\" parameters, as named above)<br>\n",
    "to the class constructors: e.g. `ramp = RampModel(beta=...)`. <br>\n",
    "(Ignore the other input arguments in the class constructor `__init__` for now, and leave them at their default values.)<br>\n",
    "Once a model object is created you can use its `simulate` method to get an array of spike trains over multiple trials. <br>\n",
    "(For usage see the docstring (or run help via `ramp.simulate?`.) `simulate` will also return the generated latent variables, <br>\n",
    "and, optionally, the firing rates in different trials. \n",
    "\n",
    "Visualise the simulated spike trains by writing code to make so-called \"spike raster\" plots. See the bottom row of Figure 5<br>\n",
    "above for example spike raster: different rows represent the spike trains in different trials, and spikes are shown by dots. <br>\n",
    "(you can put a dot for every nonzero $n_t$, even if the nonzero value is more than 1; this is unlikely if you keep `Rh` below<br>\n",
    "50 Hz and use a `T` of at least 100 (recommended). At this stage it should not be time-consuming to use higher `T`'s as well,<br>\n",
    "e.g. `T = 1000` (corresponding to 1 millisecond time-steps). If you are simulating hundreds of trials, you don't want to include<br>\n",
    "all of them in the raster. Use your common sense to decide how many trials to include in the raster; this a visualisation tool used to get<br>\n",
    "an idea of how spike trains behave qualitatively by seeing a good number of example. \n",
    "\n",
    "Vary the parameters of each model and generate spike rasters in different regions of the parameter space, trying to find<br>\n",
    "qualitatively different behavior. The default values of the parameters give you a first guess or the right order of magnitude for the <br>\n",
    "different parameters. (For `m` and `r` of the step model, note that they should scale with the `T` you will be using for the simulation;<br>\n",
    "in particular, for more interesting/relevant results, you would want to set `m` at or near `T / 2` so that the steps happen on average in the middle of the trial.)\n",
    "\n",
    "What systematic patterns can you detect? \n",
    "\n",
    "Write code to also mark the jump times in different trials over the spike trains in the raster. <br>\n",
    "Also make histograms of jump times. What is the effect of the `r` parameter on the behaiour of the stepping model?<br>\n",
    "\n",
    "Similarly make plots of the trajectories of $x_t$ or $r_t$ (of the ramp model) in several trials, in a single plot.<br>\n",
    "You can extract the time when $x_t$ of the ramping model hits its upper bound of 1 (equivalently $r_t$ reaches $R_h$), and histogram that as well.<br> \n",
    "How do `beta` and `sigma` affect this histogram or the behaviour of the $x_t$ trajectories?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e20ba518-9a6b-4eac-87f1-b3d2c7477733",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "PSTH is an important data analysis tool used in neuroscience.<br> \n",
    "This  is a statistical estimate of the trial-averaged firing rate as a function of time, based on recordings of spike trains in<br>\n",
    "multiple experimental trials. It is obtained by binning/histogramming spikes (e.g. using `np.histogram`) in different time bins<br>\n",
    "and averaging the resulting spike counts over many trials (you can also divide by `dt` to turn into rate in units of Hz).<br>\n",
    "\n",
    "Write code to construct and plot PSTH's in different regions of each model's parameter space. Note how the PSTH <br>\n",
    "fluctuates randomly from dataset to dataset. It is better to do some sort of (temporal) smoothing in order to reduce these<br>\n",
    "fluctuations and the jaggedness of the PSTH. You can use either a sliding window (e.g. a boxcar window/functin) averaging, or simply <br>\n",
    "use time bins that are larger than the oridinal time steps (e.g. 50 milliseconds -- or 5 timesteps if you are using a `dt` of 10 ms,<br> \n",
    "corresponding to `T = 100`). The smooth ramping firing rate curves in Figure 4 of the [Background](https://github.com/ahmadianlab/gg3_nda/blob/main/Background.ipynb)\n",
    "are examples of smoothed PSTH's.\n",
    "\n",
    "Even with the smoothing there will be fluctuations in the PSTH from dataset to dataset. How does the strength of these fluctuations depend on (or scale with)<br>\n",
    "the number of trials (in each dataset)? Try to be quantitative about this, e.g. by using informed plots. <br>\n",
    "For the rest of this task use a high number of trials (e.g. 5000) to minimise these fluctuations. (But note <br>\n",
    "that in real experiments the number of trials rarely exceeds a few hundred -- for for later tasks we will bring the number down.)\n",
    "\n",
    "\n",
    "Finally, try to find parameter regimes that make the PSTH of the stepping model very close to that of the ramp model. (First make sure<br>\n",
    "the ramp model's PSTH look qualitatively like the classic ramping PSTH's in LIP experiments.) In which parameter regions<br>\n",
    "does this fail drastically, and in which regimes are the two PSTH's nearly indistinguishable?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cb66917-0d16-4cb3-8946-63243c4918aa",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "The PSTH is an example of a so-called first-order statistic, in that it is the averge of spike counts, $n_t$, which is their first moment. <br>\n",
    "You can also evaluate higher order statistics, such as the variance of $n_t$ (across trials).<br>\n",
    "Instead of smoothing, for evaluating the variance use larger time bins (e.g. 50 or 100 milliseconds).\n",
    "\n",
    "How does the variance behave as a function of time and of various parameters in each model? \n",
    "\n",
    "A more useful quantity is the Fano factor which is the ratio of the variance of $n_t$ to its mean (obviously both evaluated in the same time bin, in particular<br>\n",
    "time bins of the same width). This quantity is 1 for the Poisson distribution (the default choice for the emission distribution of both models).<br>\n",
    "Evalute and plot the Fano Factor as a function of time, and again investigate how it changes in different parameter regimes, and importantly<br>\n",
    "whether and how it behaves differently in the two models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdf1821f-ef7e-436e-a443-18e45c38883f",
   "metadata": {},
   "source": [
    "### Task 1.4\n",
    "\n",
    "(This is a more open-opended and less guided task compared to the previous ones. Use brainstorming in the group and come up with creative ideas to address this, working together.)\n",
    "\n",
    "In this task you will explore an informal or relatively ad-hoc version of what we intend to do eventually using the systematic approach of Bayesian inference. <br>\n",
    "The aim is to find an intelligent but ad-hoc (in the sense that it is not Bayesian or does not rely on the models' likelihood function and only relies on \n",
    "observed statistics) way of telling the two models aparts, i.e. deciding which model generated a dataset.\n",
    "\n",
    "Relying on 1st and 2nd order statistics you have explored, or other 2nd order statistics, and perhaps higher order statistics you can come up it,\n",
    "constructing a criterion (or alternative criteria which you would compare) for deciding between the two models. To make this more challening, \n",
    "you will obviously need to put the two sets of model parameter in a regime in which they are least distinguishable from their generated spike trains. \n",
    "Use a number of trials not more than 400 for each dataset you will run your test/criterion on.\n",
    "\n",
    "You have to test your criterion by running it on several datasets, once generated by the ramp model, and in another round, generted by the step model. \n",
    "And then quantify what percent of datasets in each case where decided/classified correctly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7af18d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Week 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7794027e-ee22-44ab-9da2-d784dddedffc",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1f063ef-edaa-4096-96af-574b8594bd08",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff6ebefd-e7d3-48c4-bc96-eae663f8c4ca",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d11ef8b8-33aa-4eae-8b27-82020c6a274f",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb9b44-b77c-475c-a039-2e04cacffdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
