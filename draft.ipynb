{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316a1c9a",
   "metadata": {},
   "source": [
    "# Inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71469203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: Functions here are essentially copies of those in the\n",
    "# SSM package by Scott Linderman et al. https://github.com/lindermanlab/ssm\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from scipy.special import logsumexp as logsumexp_scipy\n",
    "from scipy.special import gammaln\n",
    "\n",
    "\n",
    "LOG_EPS = 1e-16\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, cache=True)\n",
    "def logsumexp(x):\n",
    "    N = x.shape[0]\n",
    "\n",
    "    # find the max\n",
    "    m = -np.inf\n",
    "    for i in range(N):\n",
    "        m = max(m, x[i])\n",
    "\n",
    "    # sum the exponentials\n",
    "    out = 0\n",
    "    for i in range(N):\n",
    "        out += np.exp(x[i] - m)\n",
    "        \n",
    "    return m + np.log(out)\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, cache=True)\n",
    "def dlse(a, out):\n",
    "    K = a.shape[0]\n",
    "    lse = logsumexp(a)\n",
    "    for k in range(K):\n",
    "        out[k] = np.exp(a[k] - lse)\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, cache=True)\n",
    "def forward_pass(log_pi0,\n",
    "                 log_Ps,\n",
    "                 log_likes,\n",
    "                 alphas):\n",
    "\n",
    "    T = log_likes.shape[0]  # number of time steps\n",
    "    K = log_likes.shape[1]  # number of discrete states\n",
    "\n",
    "    # if Ps.ndim == 2:\n",
    "    #     Ps = Ps[None, :, :]\n",
    "    assert log_Ps.shape[0] == T-1 or log_Ps.shape[0] == 1\n",
    "    assert log_Ps.shape[1] == K\n",
    "    assert log_Ps.shape[2] == K\n",
    "    assert alphas.shape[0] == T\n",
    "    assert alphas.shape[1] == K\n",
    "\n",
    "    # Check if we have heterogeneous transition matrices.\n",
    "    # If not, save memory by passing in log_Ps of shape (1, K, K)\n",
    "    hetero = (log_Ps.shape[0] == T-1)\n",
    "    alphas[0] = log_pi0 + log_likes[0]\n",
    "    for t in range(T-1):\n",
    "        m = np.max(alphas[t])\n",
    "        # alphas[t+1] = np.log(np.dot(np.exp(alphas[t] - m), Ps[t * hetero])) + m + log_likes[t+1]\n",
    "        alphas[t+1] = np.log(np.dot(np.exp(alphas[t] - m), np.exp(log_Ps[t * hetero]))) + m + log_likes[t+1]\n",
    "\n",
    "\n",
    "    return logsumexp(alphas[T-1])\n",
    "    for t in range(T-1):\n",
    "        m = np.max(alphas[t])\n",
    "    return logsumexp(alphas[T-1])\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, cache=True)\n",
    "def backward_pass(log_Ps,\n",
    "                  log_likes,\n",
    "                  betas):\n",
    "\n",
    "    T = log_likes.shape[0]  # number of time steps\n",
    "    K = log_likes.shape[1]  # number of discrete states\n",
    "\n",
    "    assert log_Ps.shape[0] == T-1 or log_Ps.shape[0] == 1\n",
    "    assert log_Ps.shape[1] == K\n",
    "    assert log_Ps.shape[2] == K\n",
    "    assert betas.shape[0] == T\n",
    "    assert betas.shape[1] == K\n",
    "\n",
    "    # Check if we have heterogeneous transition matrices.\n",
    "    # If not, save memory by passing in log_Ps of shape (1, K, K)\n",
    "    hetero = (log_Ps.shape[0] == T-1)\n",
    "    tmp = np.zeros(K)\n",
    "\n",
    "    # Initialize the last output\n",
    "    betas[T-1] = 0\n",
    "    for t in range(T-2,-1,-1):\n",
    "        tmp = log_likes[t+1] + betas[t+1]\n",
    "        m = np.max(tmp)\n",
    "        # betas[t] = np.log(np.dot(Ps[t * hetero], np.exp(tmp - m))) + m\n",
    "        betas[t] = np.log(np.dot( np.exp(log_Ps[t * hetero]), np.exp(tmp - m))) + m\n",
    "\n",
    "def hmm_normalizer(log_pi0, log_Ps, ll):\n",
    "    T, K = ll.shape\n",
    "    alphas = np.zeros((T, K))\n",
    "\n",
    "#     # Make sure everything is C contiguous\n",
    "#     pi0 = to_c(pi0)\n",
    "#     Ps = to_c(Ps)\n",
    "#     ll = to_c(ll)\n",
    "\n",
    "    forward_pass(log_pi0, log_Ps, ll, alphas)\n",
    "    out = logsumexp_scipy(alphas[-1])\n",
    "    return out\n",
    "\n",
    "\n",
    "def hmm_expected_states(log_pi0, log_Ps, ll, filter=False):\n",
    "    \"\"\"\n",
    "    Calculates the posterior probabilities of HMM states given the observations, implicitly input via\n",
    "    the matrix of observation log-likelihoods.\n",
    "    :param log_pi0: shape (K,),  vector of initial state probabilities (log)\n",
    "    :param log_Ps: shape (K, K): state transition matrix (time-homogeneous case), or:\n",
    "               shape (T-1, K, K): temporal sequence\n",
    "    :param ll: shape (T, K): matrix of log-likelihoods (i.e. log observation probabilities\n",
    "                             evaluated for the actual observations).\n",
    "    :param filter: False by default. If True the function calculates the so-called \"filtered\"\n",
    "                   posterior probabilities which only take into account observations until time t\n",
    "                   (as opposed to all observations until time T (with Python index T-1), which is what\n",
    "                   is calculated by default).\n",
    "    :return:\n",
    "    expected_states: this is an array of shape (T, K) with the t-th row giving the\n",
    "                     posterior probabilities of the different Markov states,\n",
    "                     conditioned on the sequence of observations.\n",
    "    normalizer: this is the model log-likelihood, i.e. it is the log-probability of the entire sequence\n",
    "                of observations (given the model parameters, which are implicit here).\n",
    "    \"\"\"\n",
    "    T, K = ll.shape\n",
    "    \n",
    "    if log_Ps.ndim == 2:\n",
    "        log_Ps = log_Ps[None, :, :]\n",
    "    assert log_Ps.ndim == 3\n",
    "\n",
    "\n",
    "    alphas = np.zeros((T, K))\n",
    "    forward_pass(log_pi0, log_Ps, ll, alphas)\n",
    "    normalizer = logsumexp(alphas[-1])\n",
    "\n",
    "    betas = np.zeros((T, K))\n",
    "    if not filter:\n",
    "        backward_pass(log_Ps, ll, betas)\n",
    "\n",
    "    # Compute P[x_t | n_{1:T}] for t = 1, ..., T (if filter = True, calculate P[x_t | n_{1:t}] instead).\n",
    "    expected_states = alphas + betas\n",
    "    expected_states -= logsumexp_scipy(expected_states, axis=1, keepdims=True)\n",
    "    expected_states = np.exp(expected_states)\n",
    "    \n",
    "    # expected_joints calculation removed\n",
    "\n",
    "    return expected_states, normalizer\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, cache=True)\n",
    "def backward_sample(log_Ps, log_likes, alphas, us, zs):\n",
    "    T = log_likes.shape[0]\n",
    "    K = log_likes.shape[1]\n",
    "    assert log_Ps.shape[0] == T-1 or log_Ps.shape[0] == 1\n",
    "    assert log_Ps.shape[1] == K\n",
    "    assert log_Ps.shape[2] == K\n",
    "    assert alphas.shape[0] == T\n",
    "    assert alphas.shape[1] == K\n",
    "    assert us.shape[0] == T\n",
    "    assert zs.shape[0] == T\n",
    "\n",
    "    lpzp1 = np.zeros(K)\n",
    "    lpz = np.zeros(K)\n",
    "\n",
    "    # Trick for handling time-varying transition matrices\n",
    "    hetero = (log_Ps.shape[0] == T-1)\n",
    "\n",
    "    for t in range(T-1,-1,-1):\n",
    "        # compute normalized log p(z[t] = k | z[t+1])\n",
    "        lpz = lpzp1 + alphas[t]\n",
    "        Z = logsumexp(lpz)\n",
    "\n",
    "        # sample\n",
    "        acc = 0\n",
    "        zs[t] = K-1\n",
    "        for k in range(K):\n",
    "            acc += np.exp(lpz[k] - Z)\n",
    "            if us[t] < acc:\n",
    "                zs[t] = k\n",
    "                break\n",
    "\n",
    "        # set the transition potential\n",
    "        if t > 0:\n",
    "            # lpzp1 = np.log(Ps[(t-1) * hetero, :, int(zs[t])] + LOG_EPS)\n",
    "            lpzp1 = log_Ps[(t-1) * hetero, :, int(zs[t])]\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, cache=True)\n",
    "def _hmm_sample(log_pi0, log_Ps, ll):\n",
    "    T, K = ll.shape\n",
    "\n",
    "    # Forward pass gets the predicted state at time t given\n",
    "    # observations up to and including those from time t\n",
    "    alphas = np.zeros((T, K))\n",
    "    forward_pass(log_pi0, log_Ps, ll, alphas)\n",
    "\n",
    "    # Sample backward\n",
    "    us = npr.rand(T)\n",
    "    zs = -1 * np.ones(T)\n",
    "    backward_sample(log_Ps, ll, alphas, us, zs)\n",
    "    return zs\n",
    "\n",
    "\n",
    "def hmm_sample(log_pi0, log_Ps, ll):\n",
    "    return _hmm_sample(log_pi0, log_Ps, ll).astype(int)\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, cache=True)\n",
    "def _viterbi(log_pi0, log_Ps, ll):\n",
    "    \"\"\"\n",
    "    This is modified from pyhsmm.internals.hmm_states\n",
    "    by Matthew Johnson.\n",
    "    \"\"\"\n",
    "    T, K = ll.shape\n",
    "\n",
    "    # Check if the transition matrices are stationary or\n",
    "    # time-varying (hetero)\n",
    "    hetero = (log_Ps.shape[0] == T-1)\n",
    "    if not hetero:\n",
    "        assert log_Ps.shape[0] == 1\n",
    "\n",
    "    # Pass max-sum messages backward\n",
    "    scores = np.zeros((T, K))\n",
    "    args = np.zeros((T, K))\n",
    "    for t in range(T-2,-1,-1):\n",
    "        # vals = np.log(Ps[t * hetero] + LOG_EPS) + scores[t+1] + ll[t+1]\n",
    "        vals = log_Ps[t * hetero] + scores[t+1] + ll[t+1]\n",
    "\n",
    "        for k in range(K):\n",
    "            args[t+1, k] = np.argmax(vals[k])\n",
    "            scores[t, k] = np.max(vals[k])\n",
    "\n",
    "    # Now maximize forwards\n",
    "    z = np.zeros(T)\n",
    "    # z[0] = (scores[0] + np.log(pi0 + LOG_EPS) + ll[0]).argmax()\n",
    "    z[0] = (scores[0] + log_pi0 + ll[0]).argmax()\n",
    "    for t in range(1, T):\n",
    "        z[t] = args[t, int(z[t-1])]\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def viterbi(log_pi0, log_Ps, ll):\n",
    "    \"\"\"\n",
    "    Find the most likely state sequence\n",
    "    \"\"\"\n",
    "    return _viterbi(log_pi0, log_Ps, ll).astype(int)\n",
    "\n",
    "\n",
    "def poisson_logpdf(counts, lambdas, mask=None):\n",
    "    \"\"\"\n",
    "    Compute the log probability of a Poisson distribution.\n",
    "    This will broadcast as long as data and lambdas have the same\n",
    "    (or at least compatible) leading dimensions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    counts : array_like of shape (Ntrials, T) or (T,)\n",
    "        array of integer counts for which to evaluate the log probability\n",
    "    lambdas : array_like of shape (K,)\n",
    "        The rates (mean counts) of the Poisson distribution(s)\n",
    "    Returns\n",
    "    -------\n",
    "    lps :  lps : array_like with shape (T, K), or (Ntrials, T, K) depending on\n",
    "          the shape of 'counts'.\n",
    "        Log probabilities under the Poisson distribution(s).\n",
    "    \"\"\"\n",
    "    assert counts.dtype in (int, np.int8, np.int16, np.int32, np.int64)\n",
    "    assert counts.ndim == 1 or counts.ndim == 2\n",
    "    if counts.ndim == 1:\n",
    "        counts = counts[:, None]\n",
    "    elif counts.ndim == 2:\n",
    "        counts = counts[:,:,None]\n",
    "\n",
    "    # Compute log pdf\n",
    "    lambdas[lambdas == 0] = 1e-8\n",
    "\n",
    "    lls = -gammaln(counts + 1) - lambdas + counts * np.log(lambdas)\n",
    "    return lls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dd727",
   "metadata": {},
   "source": [
    "# models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c66d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import *\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LOG_EPS = 1e-16\n",
    "\n",
    "\n",
    "def lo_histogram(x, bins):\n",
    "    \"\"\"\n",
    "    Left-open version of np.histogram with left-open bins covering the interval (left_edge, right_edge]\n",
    "    (np.histogram does the opposite and treats bins as right-open.)\n",
    "    Input & output behaviour is exactly the same as np.histogram\n",
    "    \"\"\"\n",
    "    out = np.histogram(-x, -bins[::-1])\n",
    "    return out[0][::-1], out[1:]\n",
    "\n",
    "\n",
    "def gamma_isi_point_process(rate, shape):\n",
    "    \"\"\"\n",
    "    Simulates (1 trial of) a sub-poisson point process (with underdispersed inter-spike intervals relative to Poisson)\n",
    "    :param rate: time-series giving the mean spike count (firing rate * dt) in different time bins (= time steps)\n",
    "    :param shape: shape parameter of the gamma distribution of ISI's\n",
    "    :return: vector of spike counts with same shape as \"rate\".\n",
    "    \"\"\"\n",
    "    sum_r_t = np.hstack((0, np.cumsum(rate)))\n",
    "    gs = np.zeros(2)\n",
    "    while gs[-1] < sum_r_t[-1]:\n",
    "        gs = np.cumsum( npr.gamma(shape, 1 / shape, size=(2 + int(2 * sum_r_t[-1]),)) )\n",
    "    y, _ = lo_histogram(gs, sum_r_t)\n",
    "\n",
    "    return y\n",
    "\n",
    "def emit(dt, rate, GammaShape=None):\n",
    "    \"\"\"\n",
    "    emit spikes based on rates\n",
    "    :param rate: firing rate sequence, r_t, possibly in many trials. Shape: (Ntrials, T)\n",
    "    :return: spike train, n_t, as an array of shape (Ntrials, T) containing integer spike counts in different\n",
    "             trials and time bins.\n",
    "    \"\"\"\n",
    "    if GammaShape is None:\n",
    "        # poisson spike emissions\n",
    "        y = npr.poisson(rate * dt)\n",
    "    else:\n",
    "        # sub-poisson/underdispersed spike emissions\n",
    "        y = gamma_isi_point_process(rate * dt, GammaShape)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "class StepModel():\n",
    "    \"\"\"\n",
    "    Simulator of the Stepping Model of Latimer et al. Science 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, m=50, r=10, x0=0.2, Rh=50, isi_gamma_shape=None, Rl=None, dt=None):\n",
    "        \"\"\"\n",
    "        Simulator of the Stepping Model of Latimer et al. Science 2015.\n",
    "        :param m: mean jump time (in # of time-steps). This is the mean parameter of the Negative Binomial distribution\n",
    "                  of jump (stepping) time\n",
    "        :param r: parameter r (\"# of successes\") of the Negative Binomial (NB) distribution of jump (stepping) time\n",
    "                  (Note that it is more customary to parametrise the NB distribution by its parameter p and r,\n",
    "                  instead of m and r, where p is so-called \"probability of success\" (see Wikipedia). The two\n",
    "                  parametrisations are equivalent and one can go back-and-forth via: m = r (1-p)/p and p = r / (m + r).)\n",
    "        :param x0: determines the pre-jump firing rate, via  R_pre = x0 * Rh (see below for Rh)\n",
    "        :param Rh: firing rate of the \"up\" state (the same as the post-jump state in most of the project tasks)\n",
    "        :param isi_gamma_shape: shape parameter of the Gamma distribution of inter-spike intervals.\n",
    "                            see https://en.wikipedia.org/wiki/Gamma_distribution\n",
    "        :param Rl: firing rate of the post-jump \"down\" state (rarely used)\n",
    "        :param dt: real time duration of time steps in seconds (only used for converting rates to units of inverse time-step)\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.r = r\n",
    "        self.x0 = x0\n",
    "\n",
    "        self.p = r / (m + r)\n",
    "\n",
    "        self.Rh = Rh\n",
    "        if Rl is not None:\n",
    "            self.Rl = Rl\n",
    "\n",
    "        self.isi_gamma_shape = isi_gamma_shape\n",
    "        self.dt = dt\n",
    "\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.m, self.r, self.x0\n",
    "\n",
    "    @property\n",
    "    def fixed_params(self):\n",
    "        return self.Rh, self.Rl\n",
    "\n",
    "\n",
    "    def emit(self, rate):\n",
    "        \"\"\"\n",
    "        emit spikes based on rates\n",
    "        :param rate: firing rate sequence, r_t, possibly in many trials. Shape: (Ntrials, T)\n",
    "        :return: spike train, n_t, as an array of shape (Ntrials, T) containing integer spike counts in different\n",
    "                 trials and time bins.\n",
    "        \"\"\"\n",
    "        if self.isi_gamma_shape is None:\n",
    "            # poisson spike emissions\n",
    "            y = npr.poisson(rate * self.dt)\n",
    "        else:\n",
    "            # sub-poisson/underdispersed spike emissions\n",
    "            y = gamma_isi_point_process(rate * self.dt, self.isi_gamma_shape)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def simulate(self, Ntrials=1, T=100, get_rate=True, GammaShape = None):\n",
    "        \"\"\"\n",
    "        :param Ntrials: (int) number of trials\n",
    "        :param T: (int) duration of each trial in number of time-steps.\n",
    "        :param get_rate: whether or not to return the rate time-series\n",
    "        :return:\n",
    "        spikes: shape = (Ntrial, T); spikes[j] gives the spike train, n_t, in trial j, as\n",
    "                an array of spike counts in each time-bin (= time step)\n",
    "        jumps:  shape = (Ntrials,) ; jumps[j] is the jump time (aka step time), tau, in trial j.\n",
    "        rates:  shape = (Ntrial, T); rates[j] is the rate time-series, r_t, in trial j (returned only if get_rate=True)\n",
    "        \"\"\"\n",
    "        if GammaShape != None:\n",
    "            self.isi_gamma_shape = GammaShape\n",
    "            \n",
    "        # set dt (time-step duration in seconds) such that trial duration is always 1 second, regardless of T.\n",
    "        dt = 1 / T\n",
    "        self.dt = dt\n",
    "\n",
    "        ts = np.arange(T)\n",
    "\n",
    "        spikes, jumps, rates = [], [], []\n",
    "        for tr in range(Ntrials):\n",
    "            # sample jump time\n",
    "            jump = npr.negative_binomial(self.r, self.p)\n",
    "            jumps.append(jump) # (unit: 1/T s )\n",
    "\n",
    "            # first set rate at all times to pre-step rate\n",
    "            rate = np.ones(T) * self.x0 * self.Rh #=R0\n",
    "            # then set rates after jump to self.Rh\n",
    "            rate[ts >= jump] = self.Rh\n",
    "            rates.append(rate)\n",
    "\n",
    "            spikes.append(self.emit(rate))\n",
    "\n",
    "        if get_rate:\n",
    "            return np.array(spikes), np.array(jumps), np.array(rates)\n",
    "        else:\n",
    "            return np.array(spikes), np.array(jumps)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def simulate_HMM_inhomo(self, Ntrials=1, T=100, get_rate=True, GammaShape = None):\n",
    "        \"\"\"\n",
    "        :param Ntrials: (int) number of trials\n",
    "        :param T: (int) duration of each trial in number of time-steps.\n",
    "        :param get_rate: whether or not to return the rate time-series\n",
    "        :return:\n",
    "        spikes: shape = (Ntrial, T); spikes[j] gives the spike train, n_t, in trial j, as\n",
    "                an array of spike counts in each time-bin (= time step)\n",
    "        jumps:  shape = (Ntrials,) ; jumps[j] is the jump time (aka step time), tau, in trial j.\n",
    "        rates:  shape = (Ntrial, T); rates[j] is the rate time-series, r_t, in trial j (returned only if get_rate=True)\n",
    "        \"\"\"\n",
    "        if GammaShape != None:\n",
    "            self.isi_gamma_shape = GammaShape\n",
    "            \n",
    "            \n",
    "        # set dt (time-step duration in seconds) such that trial duration is always 1 second, regardless of T.\n",
    "        dt = 1 / T\n",
    "        self.dt = dt\n",
    "\n",
    "        ts = np.arange(T)\n",
    "        # Inhomogeneous markov chain\n",
    "        PMF_jump = stats.nbinom.pmf(ts, self.r, self.p)\n",
    "        CMF_jump = stats.nbinom.cdf(ts, self.r, self.p)\n",
    "        \n",
    "        trans_matrices = np.empty((T-1,2,2)) # inhomogeneous transition matrix\n",
    "\n",
    "        for t in range(0,T-1):\n",
    "            if CMF_jump[t]==1:\n",
    "                trans_matrices[t] = np.array([[0, 1], [0,1]])\n",
    "            else:\n",
    "                trans_matrices[t] = np.array([[ (1-CMF_jump[t+1]) / (1-CMF_jump[t]), PMF_jump[t+1]/(1-CMF_jump[t]) ], [0,1]])\n",
    "            if np.any(trans_matrices[t] < 0):\n",
    "                print(\"invalid probability: negative value!\")\n",
    "                print(trans_matrices[t])\n",
    "                print(f\"t={t},m={self.m}, r={self.r}\")\n",
    "        # sample the first state (t=0)\n",
    "        p0 = np.array([1-PMF_jump[0],PMF_jump[0]]) \n",
    "        # logProb of t=0\n",
    "        log_pi0 = np.log(p0)\n",
    "        #log trans matrix\n",
    "        log_trans_matrices = np.log(trans_matrices)\n",
    "        \n",
    "        states = np.zeros(T, dtype=int) \n",
    "        spikes, jumps, rates = [], [], []\n",
    "        for tr in range(Ntrials):\n",
    "            # sample jump time\n",
    " \n",
    "            jump = 0\n",
    "            # Simulate the chain\n",
    "            states[0] = np.random.choice(2, size=None, replace=True, p=p0)\n",
    "            \n",
    "            # sample all other (T-1) states\n",
    "            for t in range(1, T):\n",
    "                # The transition probabilities depend on the current state\n",
    "                states[t] = np.random.choice(2, size=None, replace=True, p=trans_matrices[t-1, states[t-1]])\n",
    "                jump = t\n",
    "                if states[t]==1:\n",
    "                    jumps.append(jump) # (unit: 1/T s )\n",
    "                    break\n",
    "                elif t == T-1:\n",
    "                    jumps.append(jump+1) # Not jumped during 0:T-1\n",
    "\n",
    "            # first set rate at all times to pre-step rate\n",
    "            rate = np.ones(T) * self.x0 * self.Rh #=R0\n",
    "            # then set rates after jump to self.Rh\n",
    "            rate[ts >= jump] = self.Rh\n",
    "            rates.append(rate)\n",
    "\n",
    "            spikes.append(self.emit(rate))\n",
    "\n",
    "        if get_rate:\n",
    "            return np.array(spikes), np.array(jumps), np.array(rates), log_trans_matrices, log_pi0\n",
    "        else:\n",
    "            return np.array(spikes), np.array(jumps), log_trans_matrices, log_pi0\n",
    "  \n",
    "        \n",
    "#     def simulate_HMM_homo(self, Ntrials=1, T=100, get_rate=True):\n",
    "#         \"\"\"\n",
    "#         :param Ntrials: (int) number of trials\n",
    "#         :param T: (int) duration of each trial in number of time-steps.\n",
    "#         :param get_rate: whether or not to return the rate time-series\n",
    "#         :return:\n",
    "#         spikes: shape = (Ntrial, T); spikes[j] gives the spike train, n_t, in trial j, as\n",
    "#                 an array of spike counts in each time-bin (= time step)\n",
    "#         jumps:  shape = (Ntrials,) ; jumps[j] is the jump time (aka step time), tau, in trial j.\n",
    "#         rates:  shape = (Ntrial, T); rates[j] is the rate time-series, r_t, in trial j (returned only if get_rate=True)\n",
    "#         \"\"\"\n",
    "#         # set dt (time-step duration in seconds) such that trial duration is always 1 second, regardless of T.\n",
    "#         dt = 1 / T\n",
    "#         self.dt = dt\n",
    "\n",
    "#         ts = np.arange(T)\n",
    "        \n",
    "#         spikes, jumps, rates = [], [], []\n",
    "#         for tr in range(Ntrials):\n",
    "#             # sample jump time\n",
    "#             # homogeneous markov chain\n",
    "           \n",
    "#             # Find transition matrix for r and p\n",
    "#             trans_matrix = np.zeros((self.r+1,self.r+1))\n",
    "#             trans_matrix[0, 0:2] = [1-self.p, self.p] # Set the first row\n",
    "#             for i in range(1, self.r): # Create the remaining rows\n",
    "#                 trans_matrix[i, i:i+2] = [1-self.p, self.p]\n",
    "#             trans_matrix[self.r , self.r ] = 1 # Set the last row\n",
    "#             # Find initial distribution of latent state \n",
    "#             pi = np.zeros(self.r+1)\n",
    "#             pi[0] = 1\n",
    "#             p0 = np.matmul(pi, trans_matrix)\n",
    "#             # 1xT matrix to record states. Initial state = 0\n",
    "#             states = np.zeros(self.r+T, dtype=int) \n",
    "#             states[0] = np.random.choice((self.r+1), size=None, replace=True, p=p0)\n",
    "\n",
    "#             # sample all other (T-1) states\n",
    "#             jump=0\n",
    "#             for t in range(1, T+self.r):\n",
    "#                 # The transition probabilities depend on the current state\n",
    "#                 states[t] = np.random.choice((self.r+1), size=None, replace=True, p=trans_matrix[states[t-1]])\n",
    "#                 jump = t-self.r\n",
    "#                 if states[t]==self.r:\n",
    "#                     jumps.append(jump) #this is the jump time (ms)\n",
    "#                     states[t:] = self.r\n",
    "#                     break\n",
    "#                 elif t == T+self.r-1:\n",
    "#                     jumps.append(jump+T) # Not jumped during 0:T-1\n",
    "\n",
    "                    \n",
    "           \n",
    "#             # first set rate at all times to pre-step rate\n",
    "#             rate = np.ones(T) * self.x0 * self.Rh #=R0\n",
    "#             # then set rates after jump to self.Rh\n",
    "#             rate[ts >= jump] = self.Rh\n",
    "#             rates.append(rate)\n",
    "\n",
    "#             spikes.append(self.emit(rate))\n",
    "            \n",
    "#             # Prob of t=0\n",
    "#             log_pi0 = np.log(p0+LOG_EPS)\n",
    "#             # log tran matrix\n",
    "#             log_trans_matrix = np.log(trans_matrix+LOG_EPS)\n",
    "\n",
    "            \n",
    "#         if get_rate:\n",
    "#             return np.array(spikes), np.array(jumps), np.array(rates), log_trans_matrix, log_pi0, states\n",
    "#         else:\n",
    "#             return np.array(spikes), np.array(jumps), log_trans_matrix, log_pi0, states\n",
    "   \n",
    "    def simulate_HMM_homo(self, Ntrials=1, T=100, get_rate=True, GammaShape = None):\n",
    "        \"\"\"\n",
    "        :param Ntrials: (int) number of trials\n",
    "        :param T: (int) duration of each trial in number of time-steps.\n",
    "        :param get_rate: whether or not to return the rate time-series\n",
    "        :return:\n",
    "        spikes: shape = (Ntrial, T); spikes[j] gives the spike train, n_t, in trial j, as\n",
    "                an array of spike counts in each time-bin (= time step)\n",
    "        jumps:  shape = (Ntrials,) ; jumps[j] is the jump time (aka step time), tau, in trial j.\n",
    "        rates:  shape = (Ntrial, T); rates[j] is the rate time-series, r_t, in trial j (returned only if get_rate=True)\n",
    "        \"\"\"\n",
    "        if GammaShape != None:\n",
    "            self.isi_gamma_shape = GammaShape\n",
    "            \n",
    "        # set dt (time-step duration in seconds) such that trial duration is always 1 second, regardless of T.\n",
    "        dt = 1 / T\n",
    "        self.dt = dt\n",
    "\n",
    "        ts = np.arange(T)\n",
    "        p = self.r/(self.m+self.r)\n",
    "        \n",
    "        # homogeneous markov chain\n",
    "\n",
    "        # Find transition matrix for r and p\n",
    "        trans_matrix = np.zeros((self.r+1,self.r+1))\n",
    "        for i in range(self.r+1): # axis=0\n",
    "            for j in range(i, self.r+1): # axis=1\n",
    "                trans_matrix[i, j] = (p**(j-i)) * (1-p) if j < self.r else p**(self.r-i)\n",
    "\n",
    "        # Find initial distribution of latent state \n",
    "        pi = np.zeros(self.r+1)\n",
    "        pi[0] = 1\n",
    "        p0 = np.matmul(pi, trans_matrix)\n",
    "\n",
    "        # Prob of t=0\n",
    "        log_pi0 = np.log(p0+LOG_EPS)\n",
    "        # log tran matrix\n",
    "        log_trans_matrix = np.log(trans_matrix)\n",
    "            \n",
    "        states = np.zeros(T, dtype=int) \n",
    "        spikes, jumps, rates = [], [], []\n",
    "        for tr in range(Ntrials):\n",
    "            # sample jump time\n",
    "\n",
    "            # 1xT matrix to record states. Initial state = 0\n",
    "            states[0] = np.random.choice((self.r+1), size=None, replace=True, p=p0)\n",
    "\n",
    "            # sample all other (T-1) states\n",
    "            jump=0\n",
    "            for t in range(1, T):\n",
    "                # The transition probabilities depend on the current state\n",
    "                states[t] = np.random.choice((self.r+1), size=None, replace=True, p=trans_matrix[states[t-1]])\n",
    "                jump = t\n",
    "                if states[t]==self.r:\n",
    "                    jumps.append(jump) #this is the jump time (ms)\n",
    "                    states[t:] = self.r\n",
    "                    break\n",
    "                elif t == T-1:\n",
    "                    jumps.append(jump+T) # Not jumped during 0:T-1\n",
    "\n",
    "                    \n",
    "           \n",
    "            # first set rate at all times to pre-step rate\n",
    "            rate = np.ones(T) * self.x0 * self.Rh #=R0\n",
    "            # then set rates after jump to self.Rh\n",
    "            rate[ts >= jump] = self.Rh\n",
    "            rates.append(rate)\n",
    "\n",
    "            spikes.append(self.emit(rate))\n",
    "            \n",
    "\n",
    "            \n",
    "        if get_rate:\n",
    "            return np.array(spikes), np.array(jumps), np.array(rates), log_trans_matrix, log_pi0, states\n",
    "        else:\n",
    "            return np.array(spikes), np.array(jumps), log_trans_matrix, log_pi0, states\n",
    "        \n",
    "\n",
    "    def simulate_HMM_2states(self, Ntrials=1, T=100, get_rate=True, GammaShape = None):\n",
    "        \"\"\"\n",
    "        :param Ntrials: (int) number of trials\n",
    "        :param T: (int) duration of each trial in number of time-steps.\n",
    "        :param get_rate: whether or not to return the rate time-series\n",
    "        :return:\n",
    "        spikes: shape = (Ntrial, T); spikes[j] gives the spike train, n_t, in trial j, as\n",
    "                an array of spike counts in each time-bin (= time step)\n",
    "        jumps:  shape = (Ntrials,) ; jumps[j] is the jump time (aka step time), tau, in trial j.\n",
    "        rates:  shape = (Ntrial, T); rates[j] is the rate time-series, r_t, in trial j (returned only if get_rate=True)\n",
    "        \"\"\"\n",
    "        \n",
    "        if GammaShape != None:\n",
    "            self.isi_gamma_shape = GammaShape\n",
    "            \n",
    "        # set dt (time-step duration in seconds) such that trial duration is always 1 second, regardless of T.\n",
    "        dt = 1 / T\n",
    "        self.dt = dt\n",
    "\n",
    "        ts = np.arange(T)\n",
    "        p = self.r/(self.m+self.r)\n",
    "        \n",
    "        # homogeneous markov chain\n",
    "\n",
    "        # Find transition matrix for r and p\n",
    "        trans_matrix = np.array([[1- 1/self.m, 1/self.m],[0,1]])\n",
    "       \n",
    "\n",
    "\n",
    "        # Find initial distribution of latent state \n",
    "        pi = [1,0] # initial state\n",
    "\n",
    "        p0 = np.matmul(pi, trans_matrix)\n",
    "\n",
    "        # Prob of t=0\n",
    "        log_pi0 = np.log(p0+LOG_EPS)\n",
    "        # log tran matrix\n",
    "        log_trans_matrix = np.log(trans_matrix)\n",
    "            \n",
    "        states = np.zeros(T, dtype=int) \n",
    "        spikes, jumps, rates = [], [], []\n",
    "        for tr in range(Ntrials):\n",
    "            # sample jump time\n",
    "\n",
    "            # 1xT matrix to record states. Initial state = 0\n",
    "            states[0] = np.random.choice(2, size=None, replace=True, p=p0)\n",
    "\n",
    "            # sample all other (T-1) states\n",
    "            jump=0\n",
    "            for t in range(1, T):\n",
    "                # The transition probabilities depend on the current state\n",
    "                states[t] = np.random.choice(2, size=None, replace=True, p=trans_matrix[states[t-1]])\n",
    "                jump = t\n",
    "                if states[t]==1:\n",
    "                    jumps.append(jump) #this is the jump time (ms)\n",
    "                    states[t:] = 1\n",
    "                    break\n",
    "                elif t == T-1:\n",
    "                    jumps.append(jump+T) # Not jumped during 0:T-1\n",
    "\n",
    "                    \n",
    "           \n",
    "            # first set rate at all times to pre-step rate\n",
    "            rate = np.ones(T) * self.x0 * self.Rh #=R0\n",
    "            # then set rates after jump to self.Rh\n",
    "            rate[ts >= jump] = self.Rh\n",
    "            rates.append(rate)\n",
    "\n",
    "            spikes.append(self.emit(rate))\n",
    "            \n",
    "\n",
    "            \n",
    "        if get_rate:\n",
    "            return np.array(spikes), np.array(jumps), np.array(rates), log_trans_matrix, log_pi0, states\n",
    "        else:\n",
    "            return np.array(spikes), np.array(jumps), log_trans_matrix, log_pi0, states\n",
    "         \n",
    "        \n",
    "        \n",
    "        \n",
    "class RampModel():\n",
    "    \"\"\"\n",
    "    Simulator of the Ramping Model (aka Drift-Diffusion Model) of Latimer et al., Science (2015).\n",
    "    \"\"\"\n",
    "    def __init__(self, beta=0.5, sigma=0.2, x0=.2, Rh=50, isi_gamma_shape=None, Rl=None, dt=None):\n",
    "        \"\"\"\n",
    "        Simulator of the Ramping Model of Latimer et al. Science 2015.\n",
    "        :param beta: drift rate of the drift-diffusion process\n",
    "        :param sigma: diffusion strength of the drift-diffusion process.\n",
    "        :param x0: average initial value of latent variable x[0]\n",
    "        :param Rh: the maximal firing rate obtained when x_t reaches 1 (corresponding to the same as the post-step\n",
    "                   state in most of the project tasks)\n",
    "        :param isi_gamma_shape: shape parameter of the Gamma distribution of inter-spike intervals.\n",
    "                            see https://en.wikipedia.org/wiki/Gamma_distribution\n",
    "        :param Rl: Not implemented. Ignore.\n",
    "        :param dt: real time duration of time steps in seconds (only used for converting rates to units of inverse time-step)\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self.sigma = sigma\n",
    "        self.x0 = x0\n",
    "\n",
    "        self.Rh = Rh\n",
    "        if Rl is not None:\n",
    "            self.Rl = Rl\n",
    "\n",
    "        self.isi_gamma_shape = isi_gamma_shape\n",
    "        self.dt = dt\n",
    "\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.mu, self.sigma, self.x0\n",
    "\n",
    "    @property\n",
    "    def fixed_params(self):\n",
    "        return self.Rh, self.Rl\n",
    "\n",
    "\n",
    "    def f_io(self, xs, b=None):\n",
    "        if b is None:\n",
    "            return self.Rh * np.maximum(0, xs)\n",
    "        else:\n",
    "            return self.Rh * b * np.log(1 + np.exp(xs / b))\n",
    "\n",
    "\n",
    "    def emit(self, rate):\n",
    "        \"\"\"\n",
    "        emit spikes based on rates\n",
    "        :param rate: firing rate sequence, r_t, possibly in many trials. Shape: (Ntrials, T)\n",
    "        :return: spike train, n_t, as an array of shape (Ntrials, T) containing integer spike counts in different\n",
    "                 trials and time bins.\n",
    "        \"\"\"\n",
    "        if self.isi_gamma_shape is None:\n",
    "            # poisson spike emissions\n",
    "            y = npr.poisson(rate * self.dt)\n",
    "        else:\n",
    "            # sub-poisson/underdispersed spike emissions\n",
    "            y = gamma_isi_point_process(rate * self.dt, self.isi_gamma_shape)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def simulate(self, Ntrials=1, T=100, get_rate=True, GammaShape = None):\n",
    "        \"\"\"\n",
    "        :param Ntrials: (int) number of trials\n",
    "        :param T: (int) duration of each trial in number of time-steps.\n",
    "        :param get_rate: whether or not to return the rate time-series\n",
    "        :return:\n",
    "        spikes: shape = (Ntrial, T); spikes[j] gives the spike train, n_t, in trial j, as\n",
    "                an array of spike counts in each time-bin (= time step)\n",
    "        xs:     shape = (Ntrial, T); xs[j] is the latent variable time-series x_t in trial j\n",
    "        rates:  shape = (Ntrial, T); rates[j] is the rate time-series, r_t, in trial j (returned only if get_rate=True)\n",
    "        \"\"\"\n",
    "        \n",
    "        if GammaShape != None:\n",
    "            self.isi_gamma_shape = GammaShape\n",
    "        # set dt (time-step duration in seconds) such that trial duration is always 1 second, regardless of T.\n",
    "        dt = 1 / T\n",
    "        self.dt = dt\n",
    "\n",
    "       # simulate all trials in parallel (using numpy arrays and broadcasting)\n",
    "\n",
    "        # first, directly integrate/sum the drift-diffusion updates\n",
    "        # x[t+1] = x[t] + β dt + σ √dt * randn (with initial condition x[0] = x0 + σ √dt * randn)\n",
    "        # to get xs in shape (Ntrials, T):\n",
    "        ts = np.arange(T)\n",
    "        xs = self.x0 + self.beta * dt * ts + self.sigma * np.sqrt(dt) * np.cumsum(npr.randn(Ntrials, T), axis=1)\n",
    "        # in each trial set x to 1 after 1st passage through 1; padding xs w 1 assures passage does happen, possibly at T+1\n",
    "        taus = np.argmax(np.hstack((xs, np.ones((xs.shape[0],1)))) >= 1., axis=-1)\n",
    "        xs = np.where(ts[None,:] >= taus[:,None], 1., xs)\n",
    "        # # the above 2 lines are equivalent to:\n",
    "        # for x in xs:\n",
    "        #     if np.sum(x >= 1) > 0:\n",
    "        #         tau = np.nonzero(x >= 1)[0][0]\n",
    "        #         x[tau:] = 1\n",
    "\n",
    "        rates = self.f_io(xs) # shape = (Ntrials, T)\n",
    "\n",
    "        spikes = np.array([self.emit(rate) for rate in rates]) # shape = (Ntrial, T)\n",
    "\n",
    "        if get_rate:\n",
    "            return spikes, xs, rates\n",
    "        else:\n",
    "            return spikes, xs\n",
    "\n",
    "    def simulate_HMM(self, Ntrials=1, T=100, K=100, get_rate=True, GammaShape = None):\n",
    "        \"\"\"\n",
    "        :param Ntrials: (int) number of trials\n",
    "        :param T: (int) duration of each trial in number of time-steps.\n",
    "        :param K: (int) number of states of the HMM\n",
    "        :param get_rate: whether or not to return the rate time-series\n",
    "        :return:\n",
    "        spikes: shape = (Ntrial, T); spikes[j] gives the spike train, n_t, in trial j, as\n",
    "                an array of spike counts in each time-bin (= time step)\n",
    "        xs:     shape = (Ntrial, T); xs[j] is the latent variable time-series x_t in trial j\n",
    "        rates:  shape = (Ntrial, T); rates[j] is the rate time-series, r_t, in trial j (returned only if get_rate=True)\n",
    "        \"\"\"\n",
    "        \n",
    "        if GammaShape != None:\n",
    "            self.isi_gamma_shape = GammaShape\n",
    "        # set dt (time-step duration in seconds) such that trial duration is always 1 second, regardless of T.\n",
    "        dt = 1 / T\n",
    "        self.dt = dt\n",
    "        ts = np.arange(T)\n",
    "        \n",
    "        \n",
    "        ## transition matrix\n",
    "        # calculate difference between states, forming a K x K matrix.\n",
    "        st = np.arange(K) # states\n",
    "        s_grid = st - st.reshape(-1,1) \n",
    "        mu = self.beta * dt * (K-1)\n",
    "        std = self.sigma * np.sqrt(dt) * (K-1)  + LOG_EPS # To avoid error when sigma = 0\n",
    "\n",
    "        # Suppose trans_matrix is in log space\n",
    "        log_trans_matrix = np.log(stats.norm.cdf(s_grid+0.5, mu, std) - stats.norm.cdf(s_grid-0.5, mu, std))\n",
    "        # First column\n",
    "        log_trans_matrix[:,0] = stats.norm.logcdf(s_grid[:,0]+0.5, mu, std)\n",
    "        # Last column\n",
    "        log_trans_matrix[:,-1] = stats.norm.logsf(s_grid[:,-1]-0.5, mu, std)\n",
    "        # Last row\n",
    "        log_trans_matrix[-1] = -np.inf\n",
    "        log_trans_matrix[-1,-1] = 0\n",
    "\n",
    "        # Normalization is not necessary\n",
    "        #Compute row sums in log space using logsumexp\n",
    "        log_row_sums = np.zeros(log_trans_matrix.shape[0]) \n",
    "        for i in range(log_trans_matrix.shape[0]):\n",
    "            log_row_sums[i] = logsumexp(log_trans_matrix[i, :])\n",
    "            \n",
    "        #Subtract row sums from each element in log space to normalize\n",
    "        normalized_log_trans_matrix = log_trans_matrix - log_row_sums[:, np.newaxis]\n",
    "        trans_matrix = np.exp(normalized_log_trans_matrix)\n",
    "        \n",
    "#             # normalise each row\n",
    "#             row_sums = trans_matrix.sum(axis=1)\n",
    "#             trans_matrix = trans_matrix / row_sums[:, np.newaxis]\n",
    "\n",
    "        mu = self.x0 * (K-1)\n",
    "        std = self.sigma * np.sqrt(dt) * (K-1)  + LOG_EPS # To avoid error when sigma = 0\n",
    "        #Suppose trans_matrix is in log space\n",
    "        log_pi = np.log(stats.norm.cdf(st+0.5, mu, std) - stats.norm.cdf(st-0.5, mu, std))\n",
    "        log_pi[0] = stats.norm.logcdf(st[0]+0.5, mu, std)\n",
    "        log_pi[-1] = stats.norm.logsf(st[-1]-0.5, mu, std) # sf = 1-cdf\n",
    "        #Compute row sums in log space using logsumexp\n",
    "        log_pi_sums = logsumexp(log_pi)\n",
    "        #Subtract row sums from each element in log space to normalize\n",
    "        normalized_log_pi = log_pi - log_pi_sums\n",
    "        # recover the pi\n",
    "        pi = np.exp(normalized_log_pi)\n",
    "\n",
    "        normalized_log_pi0 = np.log(np.matmul(pi,trans_matrix))\n",
    "        # NxT matrix to record states\n",
    "        states = np.zeros((Ntrials, T), dtype=int)\n",
    "        for n in range(Ntrials):\n",
    "            # Draw the initial state from the initial distribution\n",
    "            # K states, pi initial distribution. => return a scaler with initial values (discrete)\n",
    "            states[n,0] = np.random.choice(K, p=pi)\n",
    "\n",
    "            # Simulate the chain\n",
    "            for t in range(1, T):\n",
    "                # The transition probabilities depend on the current state\n",
    "                current_state = states[n,t-1]\n",
    "                states[n,t] = np.random.choice(K, p=trans_matrix[current_state])\n",
    "\n",
    "        xs = states / (K-1)\n",
    "        \n",
    "        # in each trial set x to 1 after 1st passage through 1; padding xs w 1 assures passage does happen, possibly at T+1\n",
    "#         taus = np.argmax(np.hstack((xs, np.ones((xs.shape[0],1)))) >= 1., axis=-1)\n",
    "#         xs = np.where(ts[None,:] >= taus[:,None], 1., xs)\n",
    "\n",
    "        rates = self.f_io(xs) # shape = (Ntrials, T)\n",
    "\n",
    "        spikes = np.array([self.emit(rate) for rate in rates]) # shape = (Ntrial, T)\n",
    "\n",
    "        if get_rate:\n",
    "            return spikes, xs, rates, normalized_log_trans_matrix, normalized_log_pi0\n",
    "        else:\n",
    "            return spikes, xs, normalized_log_trans_matrix, normalized_log_pi0\n",
    "\n",
    "\n",
    "    def simulate_HMM_ns(self, Ntrials=1, num_ns=30, T=100, K=100, get_rate=True, GammaShape = None):\n",
    "        \"\"\"\n",
    "        :param Ntrials: (int) number of trials\n",
    "        :param T: (int) duration of each trial in number of time-steps.\n",
    "        :param K: (int) number of states of the HMM\n",
    "        :param get_rate: whether or not to return the rate time-series\n",
    "        :return:\n",
    "        spikes: shape = (Ntrial, T); spikes[j] gives the spike train, n_t, in trial j, as\n",
    "                an array of spike counts in each time-bin (= time step)\n",
    "        xs:     shape = (Ntrial, T); xs[j] is the latent variable time-series x_t in trial j\n",
    "        rates:  shape = (Ntrial, T); rates[j] is the rate time-series, r_t, in trial j (returned only if get_rate=True)\n",
    "        \"\"\"\n",
    "        \n",
    "        if GammaShape != None:\n",
    "            self.isi_gamma_shape = GammaShape\n",
    "        # set dt (time-step duration in seconds) such that trial duration is always 1 second, regardless of T.\n",
    "        dt = 1 / T\n",
    "        self.dt = dt\n",
    "        ts = np.arange(T)\n",
    "        \n",
    " #        num_ns = 30 number of negative states = num_ns * K\n",
    "        ## transition matrix\n",
    "        # calculate difference between states, forming a K x K matrix.\n",
    "        st = np.arange(-num_ns*K,K) # states\n",
    "        s_grid = st - st.reshape(-1,1) \n",
    "        mu = self.beta * dt * (K-1)\n",
    "        std = self.sigma * np.sqrt(dt) * (K-1)  + LOG_EPS # To avoid error when sigma = 0\n",
    "        \n",
    "        # Suppose trans_matrix is in log space\n",
    "        log_trans_matrix = np.log(stats.norm.cdf(s_grid+0.5, mu, std) - stats.norm.cdf(s_grid-0.5, mu, std))\n",
    "        # First column\n",
    "        log_trans_matrix[:,0] = stats.norm.logcdf(s_grid[:,0]+0.5, mu, std)\n",
    "        # Last column\n",
    "        log_trans_matrix[:,-1] = stats.norm.logsf(s_grid[:,-1]-0.5, mu, std)\n",
    "        # Last row\n",
    "        log_trans_matrix[-1] = -np.inf\n",
    "        log_trans_matrix[-1,-1] = 0\n",
    "\n",
    "        # Normalization is not necessary\n",
    "        #Compute row sums in log space using logsumexp\n",
    "        log_row_sums = np.zeros(log_trans_matrix.shape[0]) \n",
    "        for i in range(log_trans_matrix.shape[0]):\n",
    "            log_row_sums[i] = logsumexp(log_trans_matrix[i, :])\n",
    "            \n",
    "        #Subtract row sums from each element in log space to normalize\n",
    "        normalized_log_trans_matrix = log_trans_matrix - log_row_sums[:, np.newaxis]\n",
    "        trans_matrix = np.exp(normalized_log_trans_matrix)\n",
    "        \n",
    "#             # normalise each row\n",
    "#             row_sums = trans_matrix.sum(axis=1)\n",
    "#             trans_matrix = trans_matrix / row_sums[:, np.newaxis]\n",
    "\n",
    "        mu = self.x0 * (K-1)\n",
    "        std = self.sigma * np.sqrt(dt) * (K-1)  + LOG_EPS # To avoid error when sigma = 0\n",
    "        #Suppose trans_matrix is in log space\n",
    "        log_pi = np.log(stats.norm.cdf(st+0.5, mu, std) - stats.norm.cdf(st-0.5, mu, std))\n",
    "        log_pi[0] = stats.norm.logcdf(st[0]+0.5, mu, std)\n",
    "        log_pi[-1] = stats.norm.logsf(st[-1]-0.5, mu, std) # sf = 1-cdf\n",
    "\n",
    "        #Compute normalized_log_pi in log space using logsumexp\n",
    "        normalized_log_pi = log_pi - logsumexp(log_pi)\n",
    "        # recover the pi\n",
    "        pi = np.exp(normalized_log_pi)\n",
    "        \n",
    "        normalized_log_pi0 = np.log(np.matmul(pi,trans_matrix))\n",
    "\n",
    "\n",
    "        # NxT matrix to record states\n",
    "        states = np.zeros((Ntrials, T), dtype=int)\n",
    "        for n in range(Ntrials):\n",
    "            # Draw the initial state from the initial distribution\n",
    "            # K states, pi initial distribution. => return a scaler with initial values (discrete)\n",
    "            states[n,0] = np.random.choice(st, p=pi)\n",
    "            # Simulate the chain\n",
    "            for t in range(1, T):\n",
    "                # The transition probabilities depend on the current state\n",
    "                current_state = states[n,t-1]\n",
    "                states[n,t] = np.random.choice(st, p=trans_matrix[current_state+num_ns*K])\n",
    "\n",
    "        xs = states / (K-1)\n",
    "        \n",
    "        # in each trial set x to 1 after 1st passage through 1; padding xs w 1 assures passage does happen, possibly at T+1\n",
    "#         taus = np.argmax(np.hstack((xs, np.ones((xs.shape[0],1)))) >= 1., axis=-1)\n",
    "#         xs = np.where(ts[None,:] >= taus[:,None], 1., xs)\n",
    "\n",
    "        rates = self.f_io(xs) # shape = (Ntrials, T)\n",
    "\n",
    "        spikes = np.array([self.emit(rate) for rate in rates]) # shape = (Ntrial, T)\n",
    "\n",
    "        if get_rate:\n",
    "            return spikes, xs, rates, normalized_log_trans_matrix, normalized_log_pi0, states\n",
    "        else:\n",
    "            return spikes, xs, normalized_log_trans_matrix, normalized_log_pi0, states\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf7635",
   "metadata": {},
   "source": [
    "# models2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from models import *\n",
    "from inference import *\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"divide by zero encountered in log\")\n",
    "\n",
    "# def MLR_classifier(data_points):\n",
    "# #     :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix\n",
    "    \n",
    "    \n",
    "#     M=26\n",
    "#     K=100\n",
    "#     logMLR = MLR_calculator(counts_matrix, M = M, K = K, Print = False)\n",
    "    \n",
    "#     if\n",
    "\n",
    "\n",
    "def MLR_classifier(data_points):\n",
    "    \"\"\"\n",
    "    Classify spike trains as being generated by the step model (return 0) or the ramp model (return 1).\n",
    "    :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix\n",
    "    :param m: mean jump time (in # of time-steps) for StepModel\n",
    "    :param r: parameter r (\"# of successes\") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel\n",
    "    :param sigma: diffusion strength of the drift-diffusion process for RampModel\n",
    "    :param beta: drift rate of the drift-diffusion process for RampModel\n",
    "    :param threshold: threshold for variance\n",
    "    :return: M predictions, each being 0 (step model) or 1 (ramp model)\n",
    "    \"\"\"\n",
    "    predictions = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "    logMLRs = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "    logMLR=0\n",
    "    \n",
    "    for ii in [0,1]:\n",
    "        # ii = 0 -> STEP spike trains\n",
    "        # ii = 1 -> RAMP spike trains\n",
    "        for jj in range(data_points.shape[1]): \n",
    "            spike_trains = data_points[ii, jj]; # (N by T) spike train matrix\n",
    "            # Calculate the PSTH\n",
    "            counts_matrix= generate_psth(spike_trains, return_counts=True)\n",
    "            logMLR, _, _ = MLR_calculator(counts_matrix, M = 10, K = 10, Print=False)\n",
    "            logMLRs[ii,jj] = logMLR\n",
    "    predictions = np.where(logMLRs > 0, 1, 0)\n",
    "\n",
    "    return predictions, logMLRs\n",
    "\n",
    "\n",
    "def MLR_calculator(counts_matrix, M = 26, K = 100, Print = False):\n",
    "# read counts_matrix (N x T） and return MLR\n",
    "    start_time = time.time()\n",
    "\n",
    "    N = counts_matrix.shape[0]\n",
    "    ## Setup ##\n",
    "\n",
    "    values_r = np.linspace(1, 10, M)\n",
    "    values_m = np.linspace(0, 100, M)\n",
    "    values_logs = np.linspace(np.log(0.04), np.log(4), M) # -3.22 - 1.386\n",
    "    values_b = np.linspace(0, 4, M)\n",
    "    values_x0 = np.linspace(0, 1, M)\n",
    "\n",
    "    T=100\n",
    "    time_points = np.linspace(1,T,T) # 0,1,2,...\n",
    "    dt = 1/T\n",
    "    time_ms = time_points * dt * 1e3\n",
    "\n",
    "    Rh = 50\n",
    "    bin_size = 20\n",
    "    bin_size_2 = 50\n",
    "    bin_edges = np.arange(0, 1e3+bin_size, bin_size)\n",
    "    st = np.arange(K) # states\n",
    "    xt = st/(K-1)\n",
    "\n",
    "    ## inference marginal ll for both model ##\n",
    "\n",
    "    # Ramp MLL\n",
    "    log_prior = np.log(1/M**3)\n",
    "    model_ll = np.zeros((M,M,M))\n",
    "\n",
    "    for b_idx in range(M):\n",
    "        for s_idx in range(M):\n",
    "            for x_idx in range(M):\n",
    "\n",
    "                b = values_b[b_idx]\n",
    "                logs = values_logs[s_idx]\n",
    "                x0 = values_x0[x_idx]\n",
    "\n",
    "                ramp = RampModel(beta=b, sigma=np.exp(logs), x0=x0, Rh=50)\n",
    "                [_, _, _, normalized_log_trans_matrix, normalized_log_pi0] = ramp.simulate_HMM(Ntrials=0, \n",
    "                                                                                               T=T, \n",
    "                                                                                               K=K, \n",
    "                                                                                               get_rate=True)\n",
    "                normalized_log_trans_matrix = normalized_log_trans_matrix[np.newaxis, :, :] # (1,K,K) for homogeneous MC\n",
    "\n",
    "                lls = poisson_logpdf(counts=counts_matrix, lambdas= xt*Rh*dt, mask=None) # N x T x K \n",
    "\n",
    "                # Model log-likelihood for N trials:\n",
    "                for n in range(N):\n",
    "                    model_ll[b_idx, s_idx, x_idx] += hmm_normalizer(log_pi0 = normalized_log_pi0, \n",
    "                                                                    log_Ps = normalized_log_trans_matrix, \n",
    "                                                                    ll = lls[n])\n",
    "\n",
    "    # convert all nan to -inf\n",
    "    model_ll = np.nan_to_num(model_ll, nan=-np.inf)\n",
    "\n",
    "    # Model log-posterior\n",
    "    unnormalised_log_poste = model_ll + log_prior\n",
    "    # print(unnormalised_log_poste)\n",
    "\n",
    "    ramp_MLL = logsumexp_scipy(unnormalised_log_poste)\n",
    "    # log_poste = unnormalised_log_poste - ramp_MLL\n",
    "    # poste = np.exp(log_poste)+1e-16\n",
    "\n",
    "    if Print:\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(\"Ramp marginal log-likelihood inferred：\", elapsed_time, \"s\")     \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step MLL\n",
    "    log_prior = np.log(1/M**3)\n",
    "    model_ll = np.zeros((M,M,M))\n",
    "    for m_idx in range(M):\n",
    "        for r_idx in range(M):\n",
    "            for x_idx in range(M):\n",
    "                m = values_m[m_idx]\n",
    "                r = values_r[r_idx]\n",
    "                x0 = values_x0[x_idx]\n",
    "\n",
    "                step = StepModel(r=r, m=m, x0=x0, Rh=Rh)\n",
    "                [_, _, _, normalized_log_trans_matrix, normalized_log_pi0] = step.simulate_HMM_inhomo(Ntrials=0, \n",
    "                                                                                                      T=T, \n",
    "                                                                                                      get_rate=True)\n",
    "\n",
    "                rt = np.array([x0, 1]) * Rh * dt\n",
    "                lls = poisson_logpdf(counts=counts_matrix,lambdas= rt, mask=None) # N x T x K \n",
    "\n",
    "                if np.isnan(lls).any():\n",
    "                    print(lls)\n",
    "\n",
    "                # Model log-likelihood for N trials:\n",
    "                for n in range(N):\n",
    "                    model_ll[m_idx, r_idx, x_idx] += hmm_normalizer(log_pi0 = normalized_log_pi0, \n",
    "                                                                    log_Ps = normalized_log_trans_matrix, \n",
    "                                                                    ll = lls[n])\n",
    "                if np.isnan(model_ll[m_idx, r_idx, x_idx]):\n",
    "                    print(normalized_log_trans_matrix)\n",
    "                    print(lls)\n",
    "\n",
    "    # convert all nan to -inf\n",
    "    model_ll = np.nan_to_num(model_ll, nan=-np.inf)\n",
    "\n",
    "    # Model log-posterior\n",
    "    unnormalised_log_poste = model_ll + log_prior\n",
    "    # print(unnormalised_log_poste)\n",
    "\n",
    "    step_MLL = logsumexp_scipy(unnormalised_log_poste)\n",
    "    # log_poste = unnormalised_log_poste - step_MLL\n",
    "    # poste = np.exp(log_poste) + 1e-16\n",
    "    if Print:\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(\"Step marginal log-likelihood inferred：\", elapsed_time, \"s\") \n",
    "\n",
    "\n",
    "    logMLR = ramp_MLL - step_MLL\n",
    "    return logMLR, ramp_MLL, step_MLL\n",
    "\n",
    "\n",
    "def classifier_tester(M=100, N=400, T=100, classifier=\"mlr\", thresholds=None):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data_points = generate_test_spike_trains(M=M, N=N, T=T, model=\"original\")\n",
    "    # r, b, s are controled by exponent\n",
    "    # m, x are generated linearily\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"2x{M} datasets generated (NxT = {N}x{T})：\", elapsed_time, \"s\")     \n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    if classifier == \"var\":\n",
    "        predictions, _, _= var_classifier(data_points, thresholds)\n",
    "    if classifier == \"mlr\":\n",
    "        predictions, _ = MLR_classifier(data_points)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Pridictions generated：\", elapsed_time, \"s\")     \n",
    "\n",
    "    accuracy = (np.sum(1-predictions[0]) + np.sum(predictions[1]))/(2*M)\n",
    "    return accuracy\n",
    "\n",
    "def generate_test_spike_trains(num_grid=10, M=20, N=400, T=100, rmin=1, rmax=100, bmin=0, bmax=4, logsmin=0.04, logsmax=4, mmin=0, mmax=100, xmin=0, xmax=1, model=\"original\", GammaShape=None):\n",
    "    \"\"\"\n",
    "    Generate M data points for both ramp model and step model\n",
    "    :param M: number of data points for each model\n",
    "    :param N: number of trls per data point\n",
    "    :param T: duration of each trial in number of time-steps\n",
    "    :param m: mean jump time (in # of time-steps) for StepModel\n",
    "    :param r: parameter r (\"# of successes\") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel\n",
    "    :param sigma: diffusion strength of the drift-diffusion process for RampModel\n",
    "    :param beta: drift rate of the drift-diffusion process for RampModel\n",
    "    :return: A matrix with dim (2, M, N, T)\n",
    "    \"\"\"\n",
    "    \n",
    "       \n",
    "    # Generate M data points\n",
    "    data_points = np.empty((2, M, N, T))  # for an n x m array\n",
    "    for MM in range(M):\n",
    "  # Initialize random model parameters\n",
    "        m = npr.uniform(mmin, mmax)\n",
    "        r = npr.uniform(rmin, rmax)\n",
    "        b = npr.uniform(bmin,bmax)\n",
    "        s = np.exp(npr.uniform(logsmin,logsmax))\n",
    "        xr = npr.uniform(xmin,xmax)\n",
    "        xs = npr.uniform(xmin,xmax)\n",
    "        \n",
    "        \n",
    "        #initialise models\n",
    "        step_model = StepModel(m=m, r=r, x0=xs, Rh=50);\n",
    "        ramp_model = RampModel(beta=b, sigma=s, x0=xr, Rh=50);\n",
    "\n",
    "        # Generate spike trains\n",
    "        if model == \"original\":\n",
    "            step_spikes, _, _ = step_model.simulate(Ntrials=N, T=T, GammaShape = GammaShape)\n",
    "            ramp_spikes, _, _ = ramp_model.simulate(Ntrials=N, T=T, GammaShape = GammaShape)\n",
    "        elif model == \"hmm\":\n",
    "            # To be added here\n",
    "            step_spikes, _, _,_ ,_= step_model.simulate_HMM_inhomo(Ntrials=N, T=T, GammaShape = GammaShape)\n",
    "            ramp_spikes, _, _,_,_ = ramp_model.simulate_HMM(Ntrials=N, T=T, K=100, GammaShape = GammaShape)\n",
    "\n",
    "        # Add spike trains to data points\n",
    "        data_points[0,MM]=step_spikes\n",
    "        data_points[1,MM]=ramp_spikes\n",
    "\n",
    "  \n",
    "    # Convert data_points to integer type to save memory\n",
    "    data_points = data_points.astype(int)\n",
    "    return data_points\n",
    "\n",
    "def generate_spike_trains(M=20, N=400, T=100, m=50, r=10, sigma=0.2, beta=0.5):\n",
    "    \"\"\"\n",
    "    Generate M data points for both ramp model and step model\n",
    "    :param M: number of data points for each model\n",
    "    :param N: number of trls per data point\n",
    "    :param T: duration of each trial in number of time-steps\n",
    "    :param m: mean jump time (in # of time-steps) for StepModel\n",
    "    :param r: parameter r (\"# of successes\") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel\n",
    "    :param sigma: diffusion strength of the drift-diffusion process for RampModel\n",
    "    :param beta: drift rate of the drift-diffusion process for RampModel\n",
    "    :return: A matrix with dim (2, M, N, T)\n",
    "    \"\"\"\n",
    "    # Initialize models\n",
    "    step_model = StepModel(m=m, r=r, x0=0.2, Rh=50);\n",
    "    ramp_model = RampModel(beta=beta, sigma=sigma);\n",
    "\n",
    "    # Generate M data points\n",
    "    data_points = np.empty((2, M, N, T))  # for an n x m array\n",
    "    for MM in range(M):\n",
    "        # Generate spike trains\n",
    "        step_spikes, _, _ = step_model.simulate(Ntrials=N, T=T)\n",
    "        ramp_spikes, _, _ = ramp_model.simulate(Ntrials=N, T=T)\n",
    "\n",
    "        # Add spike trains to data points\n",
    "        data_points[0,MM]=step_spikes\n",
    "        data_points[1,MM]=ramp_spikes\n",
    "\n",
    "\n",
    "    # Convert data_points to integer type to save memory\n",
    "    data_points = data_points.astype(int)\n",
    "\n",
    "    return data_points\n",
    "\n",
    "\n",
    "def generate_raster_and_timestamps(spike_trains, plot=False):\n",
    "    \"\"\"\n",
    "    Generate a raster plot and timestamps of the given spike trains.\n",
    "    :param spike_trains: spike trains to plot (N by T matrix)\n",
    "    :param plot: whether to plot the raster\n",
    "    :return: spike trains timestamps\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    T = len(spike_trains[0])\n",
    "    # Record time of spikes in milliseconds\n",
    "    spike_trains_timestamp = []\n",
    "    for spike_train in spike_trains:  # for each trial\n",
    "        timestamp = []\n",
    "#         print(spike_train)\n",
    "        for ii in range(len(spike_train)):  # for each time point\n",
    "#             print(spike_train[ii])\n",
    "            for jj in range(spike_train[ii]):  # handle multiple spikes in a time stamp\n",
    "                timestamp.append(ii*1e3/T)\n",
    "        spike_trains_timestamp.append(timestamp)\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.suptitle(\"Spike Raster Plot\")\n",
    "        colors = ['C{}'.format(i) for i in range(len(spike_trains))]  # different color for each set of neural data\n",
    "        ax.eventplot(spike_trains_timestamp, colors=colors, linelengths=0.2)\n",
    "        ax.yaxis.set_tick_params(labelleft=False)\n",
    "        ax.set_xlabel(\"time from motion onset (ms)\")\n",
    "        ax.set_ylabel(\"spike trains\")\n",
    "        plt.show()\n",
    "\n",
    "    return spike_trains_timestamp\n",
    "\n",
    "\n",
    "def generate_psth(spike_trains, bin_size=20, bin_size_2=50, plot=False, return_counts=False):\n",
    "    \"\"\"\n",
    "    Generate a Peri-Stimulus Time Histogram (PSTH) from given timestamps.\n",
    "    :param spike_trains: spike trains to plot (N by T matrix)\n",
    "    :param bin_size: bin size for the PSTH (in milliseconds)\n",
    "    :param bin_size2: a larger size to calculate the variance of psth (in milliseconds)\n",
    "    :param plot: whether to plot the PSTH\n",
    "    :return: averaged PSTH, smoothed_psth, variance, Fano factor\n",
    "    \"\"\"\n",
    "    \n",
    "    N = spike_trains.shape[0]; # number of trials\n",
    "    T = spike_trains.shape[1];\n",
    "    #print(N)\n",
    "    spike_trains_timestamp = generate_raster_and_timestamps(spike_trains); # timestamps of spike trains\n",
    "    \n",
    "    \n",
    "    if return_counts == True:\n",
    "        counts_matrix = np.zeros((N, T)); # (N x T）\n",
    "        # Calculate the PSTH for each trail\n",
    "        for ii in range(len(spike_trains_timestamp)):\n",
    "            bin_edges_for_counts = np.arange(0, 1e3 + 1000/T, 1000/T)\n",
    "            counts_matrix[ii], _ = np.histogram(np.array(spike_trains_timestamp[ii]), bins=bin_edges_for_counts)\n",
    "        counts_matrix = counts_matrix.astype(int)\n",
    "        return counts_matrix\n",
    "    \n",
    "    \n",
    "    # Calculate the PSTH\n",
    "    bin_edges = np.arange(0, 1e3 + bin_size, bin_size)\n",
    "    psth, _ = np.histogram(np.concatenate(spike_trains_timestamp), bins=bin_edges)\n",
    "\n",
    "    averaged_psth = (psth / bin_size * 1e3) / N # spikes per sec per trail\n",
    "\n",
    "    # Apply Gaussian smoothing\n",
    "    sigma = 1.5  # Standard deviation of the Gaussian filter\n",
    "    gaussian_smoothed_psth = gaussian_filter(averaged_psth, sigma)\n",
    "\n",
    "    # Calculate the PSTH for larger bins\n",
    "    bin_edges_2 = np.arange(0, 1e3+bin_size_2, bin_size_2)\n",
    "    psth_2, _ = np.histogram(np.concatenate(spike_trains_timestamp), bins=bin_edges_2)\n",
    "    averaged_psth_2 = psth_2 / N # spikes per trail\n",
    "\n",
    "    var_s = np.zeros_like(averaged_psth_2)\n",
    "\n",
    "    # psth_matrix is a 2D numpy array where each row is a PSTH vector\n",
    "    psth_matrix = np.zeros((N, len(averaged_psth_2))); # (N x time_bins)\n",
    "\n",
    "    # Calculate the PSTH for each trail\n",
    "    for ii in range(len(spike_trains_timestamp)):\n",
    "        psth_matrix[ii], _ = np.histogram(np.array(spike_trains_timestamp[ii]), bins=bin_edges_2)\n",
    "    \n",
    "    \n",
    "#     print(psth_matrix.shape)\n",
    "    # Find the variance across trials (i.e., along the rows)\n",
    "    var_s = np.var(psth_matrix, axis=0);\n",
    "\n",
    "    ## Calculate Fano Factor ##\n",
    "\n",
    "    fano_factors = var_s / averaged_psth_2\n",
    "\n",
    "\n",
    "    if plot:\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "        fig.suptitle(\"PSTH Diagram\")\n",
    "\n",
    "        # Plot the PSTH\n",
    "        ax1.plot(bin_edges[:-1], averaged_psth,  label='Original')\n",
    "        ax1.plot(bin_edges[:-1], gaussian_smoothed_psth,  label='Smoothed')\n",
    "        ax2.plot(bin_edges_2[:-1], var_s,  label='Variance')\n",
    "        ax3.plot(bin_edges_2[:-1], fano_factors,  label='Fano factor')\n",
    "\n",
    "        ax1.set_ylabel(\"spike rate (sp/s)\")\n",
    "        ax2.set_ylabel(\"Variance\")\n",
    "        ax3.set_ylabel(\"Fano factor\")\n",
    "        ax3.set_xlabel(\"time from motion onset (ms)\")\n",
    "        ax1.legend()\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return averaged_psth, gaussian_smoothed_psth, var_s, fano_factors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def var_classifier(data_points, thresholds):\n",
    "    \"\"\"\n",
    "    Classify spike trains as being generated by the step model (return 0) or the ramp model (return 1).\n",
    "    :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix\n",
    "    :param m: mean jump time (in # of time-steps) for StepModel\n",
    "    :param r: parameter r (\"# of successes\") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel\n",
    "    :param sigma: diffusion strength of the drift-diffusion process for RampModel\n",
    "    :param beta: drift rate of the drift-diffusion process for RampModel\n",
    "    :param threshold: threshold for variance\n",
    "    :return: M predictions, each being 0 (step model) or 1 (ramp model)\n",
    "    \"\"\"\n",
    "    predictions = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "    var_s = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "    fano_factors = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "    \n",
    "    for ii in [0,1]:\n",
    "        # ii = 0 -> STEP spike trains\n",
    "        # ii = 1 -> RAMP spike trains\n",
    "        for jj in range(data_points[ii].shape[0]): \n",
    "            spike_trains = data_points[ii, jj]; # (N by T) spike train matrix\n",
    "            # Calculate the PSTH\n",
    "            _,psth,_,_ = generate_psth(spike_trains, bin_size=20, bin_size_2=50)\n",
    "\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            # Scale the PSTH \n",
    "    #         psth_step_scaled = psth_step * 2 * m / len(psth_step)\n",
    "    #         psth_ramp_scaled = psth_ramp * 2 * m / len(psth_ramp)\n",
    "\n",
    "            # Find the gradient of the PSTH\n",
    "            grad_psth = np.gradient(psth)\n",
    "\n",
    "            # Find the variance and the Fano factor of the gradient\n",
    "            var = np.var(grad_psth)\n",
    "            fano_factor = var / np.mean(grad_psth)\n",
    "            \n",
    "            # Print the variance and the Fano factor\n",
    "            # print(f\"variance = {var}, Fano factor = {fano_factor}\")\n",
    "            var_s[ii,jj] = var\n",
    "            fano_factors[ii,jj] = fano_factor\n",
    "\n",
    "            # Classify the spike trains based on the variance \n",
    "            if var > thresholds:\n",
    "                predictions[ii, jj] = 0  # step model\n",
    "            else:\n",
    "                predictions[ii, jj] = 1  # ramp model\n",
    "\n",
    "#             if var_ramp > threshold:\n",
    "#                 predictions.append(0)  # step model\n",
    "#             else:\n",
    "#                 predictions.append(1)  # ramp model\n",
    "\n",
    "    return predictions, var_s, fano_factors\n",
    "\n",
    "def higher_order_classifier(data_points, thresholds):\n",
    "    \"\"\"\n",
    "    Classify spike trains as being generated by the step model (return 0) or the ramp model (return 1).\n",
    "    :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix\n",
    "    :param m: mean jump time (in # of time-steps) for StepModel\n",
    "    :param r: parameter r (\"# of successes\") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel\n",
    "    :param sigma: diffusion strength of the drift-diffusion process for RampModel\n",
    "    :param beta: drift rate of the drift-diffusion process for RampModel\n",
    "    :param threshold: threshold for variance\n",
    "    :return: M predictions, each being 0 (step model) or 1 (ramp model)\n",
    "    \"\"\"\n",
    "    predictions = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "    var_s = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "    fano_factors = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "    \n",
    "    for ii in [0,1]:\n",
    "        # ii = 0 -> STEP spike trains\n",
    "        # ii = 1 -> RAMP spike trains\n",
    "        for jj in range(data_points[ii].shape[0]): \n",
    "            spike_trains = data_points[ii, jj]; # (N by T) spike train matrix\n",
    "            # Calculate the PSTH\n",
    "            _,psth,_,_ = generate_psth(spike_trains, bin_size=20, bin_size_2=50)\n",
    "            \n",
    "            \n",
    "            ### find the vilid region\n",
    "            \n",
    "            \n",
    "            \n",
    "            ### Gradient and average gradient\n",
    "\n",
    "            # Find the gradient of the PSTH\n",
    "            grad_psth = np.gradient(psth)\n",
    "            average_grad = (psth[-1] - psth[1]) / len(psth)\n",
    "            # Find the variance and the Fano factor of the gradient\n",
    "            var = np.var(grad_psth)\n",
    "            fano_factor = var / np.mean(grad_psth)\n",
    "            \n",
    "            # Print the variance and the Fano factor\n",
    "            # print(f\"variance = {var}, Fano factor = {fano_factor}\")\n",
    "            var_s[ii,jj] = var\n",
    "            fano_factors[ii,jj] = fano_factor\n",
    "\n",
    "            # Classify the spike trains based on the variance \n",
    "            if var > thresholds:\n",
    "                predictions[ii, jj] = 0  # step model\n",
    "            else:\n",
    "                predictions[ii, jj] = 1  # ramp model\n",
    "\n",
    "#             if var_ramp > threshold:\n",
    "#                 predictions.append(0)  # step model\n",
    "#             else:\n",
    "#                 predictions.append(1)  # ramp model\n",
    "\n",
    "    return predictions, var_s, fano_factors\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "# def normalised_var_classifier(data_points, m, r, sigma, beta, threshold):\n",
    "#     \"\"\"\n",
    "#     Classify spike trains as being generated by the step model (return 0) or the ramp model (return 1).\n",
    "#     :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix\n",
    "#     :param m: mean jump time (in # of time-steps) for StepModel\n",
    "#     :param r: parameter r (\"# of successes\") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel\n",
    "#     :param sigma: diffusion strength of the drift-diffusion process for RampModel\n",
    "#     :param beta: drift rate of the drift-diffusion process for RampModel\n",
    "#     :param threshold: threshold for variance\n",
    "#     :return: M predictions, each being 0 (step model) or 1 (ramp model)\n",
    "#     \"\"\"\n",
    "#     predictions = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "#     var_s = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "#     fano_factors = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M\n",
    "    \n",
    "#     for ii in [0,1]:\n",
    "#         # ii = 0 -> STEP spike trains\n",
    "#         # ii = 1 -> RAMP spike trains\n",
    "#         for jj in range(data_points[ii].shape[0]): \n",
    "#             spike_trains = data_points[ii, jj]; # (N by T) spike train matrix\n",
    "#             # Calculate the PSTH\n",
    "#             psth,_,_ = generate_psth(spike_trains, bin_size=20, bin_size_2=50)\n",
    "\n",
    "#             #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#             # Scale the PSTH \n",
    "#     #         psth_step_scaled = psth_step * 2 * m / len(psth_step)\n",
    "#     #         psth_ramp_scaled = psth_ramp * 2 * m / len(psth_ramp)\n",
    "\n",
    "#             # Find the gradient of the PSTH\n",
    "#             grad_psth = np.gradient(psth)\n",
    "            \n",
    "#             # Normalize the gradient so that the area under it is equal to 1\n",
    "#             grad_psth_normalized = grad_psth / np.sum(grad_psth)\n",
    "\n",
    "#             # Find the variance and the Fano factor of the gradient\n",
    "#             var = np.var(grad_psth_normalized)\n",
    "#             fano_factor = var / np.mean(grad_psth_normalized)\n",
    "            \n",
    "            \n",
    "#             # Print the variance and the Fano factor\n",
    "#             # print(f\"variance = {var}, Fano factor = {fano_factor}\")\n",
    "#             var_s[ii,jj] = var\n",
    "#             fano_factors[ii,jj] = fano_factor\n",
    "#             #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#             # Classify the spike trains based on the variance \n",
    "# #             if var > threshold:\n",
    "# #                 predictions[ii,].append(0)  # step model\n",
    "# #             else:\n",
    "# #                 predictions[ii].append(1)  # ramp model\n",
    "\n",
    "# #             if var_ramp > threshold:\n",
    "# #                 predictions.append(0)  # step model\n",
    "# #             else:\n",
    "# #                 predictions.append(1)  # ramp model\n",
    "\n",
    "#     return predictions, var_s, fano_factors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
