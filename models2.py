import numpy as np
import numpy.random as npr
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter
from models import *
from inference import *
import time
import warnings
warnings.filterwarnings("ignore", "divide by zero encountered in log")

# def MLR_classifier(data_points):
# #     :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix
    
    
#     M=26
#     K=100
#     logMLR = MLR_calculator(counts_matrix, M = M, K = K, Print = False)
    
#     if


def MLR_classifier(data_points):
    """
    Classify spike trains as being generated by the step model (return 0) or the ramp model (return 1).
    :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix
    :param m: mean jump time (in # of time-steps) for StepModel
    :param r: parameter r ("# of successes") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel
    :param sigma: diffusion strength of the drift-diffusion process for RampModel
    :param beta: drift rate of the drift-diffusion process for RampModel
    :param threshold: threshold for variance
    :return: M predictions, each being 0 (step model) or 1 (ramp model)
    """
    predictions = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
    logMLRs = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
    logMLR=0
    
    for ii in [0,1]:
        # ii = 0 -> STEP spike trains
        # ii = 1 -> RAMP spike trains
        for jj in range(data_points.shape[1]): 
            spike_trains = data_points[ii, jj]; # (N by T) spike train matrix
            # Calculate the PSTH
            counts_matrix= generate_psth(spike_trains, return_counts=True)
            logMLR, _, _ = MLR_calculator(counts_matrix, M = 10, K = 10, Print=False)
            logMLRs[ii,jj] = logMLR
    predictions = np.where(logMLRs > 0, 1, 0)

    return predictions, logMLRs


def MLR_calculator(counts_matrix, M = 26, K = 100, Print = False):
# read counts_matrix (N x T） and return MLR
    start_time = time.time()

    N = counts_matrix.shape[0]
    ## Setup ##

    values_r = np.linspace(1, 10, M)
    values_m = np.linspace(0, 100, M)
    values_logs = np.linspace(np.log(0.04), np.log(4), M) # -3.22 - 1.386
    values_b = np.linspace(0, 4, M)
    values_x0 = np.linspace(0, 1, M)

    T=100
    time_points = np.linspace(1,T,T) # 0,1,2,...
    dt = 1/T
    time_ms = time_points * dt * 1e3

    Rh = 50
    bin_size = 20
    bin_size_2 = 50
    bin_edges = np.arange(0, 1e3+bin_size, bin_size)
    st = np.arange(K) # states
    xt = st/(K-1)

    ## inference marginal ll for both model ##

    # Ramp MLL
    log_prior = np.log(1/M**3)
    model_ll = np.zeros((M,M,M))

    for b_idx in range(M):
        for s_idx in range(M):
            for x_idx in range(M):

                b = values_b[b_idx]
                logs = values_logs[s_idx]
                x0 = values_x0[x_idx]

                ramp = RampModel(beta=b, sigma=np.exp(logs), x0=x0, Rh=50)
                [_, _, _, normalized_log_trans_matrix, normalized_log_pi0] = ramp.simulate_HMM(Ntrials=0, 
                                                                                               T=T, 
                                                                                               K=K, 
                                                                                               get_rate=True)
                normalized_log_trans_matrix = normalized_log_trans_matrix[np.newaxis, :, :] # (1,K,K) for homogeneous MC

                lls = poisson_logpdf(counts=counts_matrix, lambdas= xt*Rh*dt, mask=None) # N x T x K 

                # Model log-likelihood for N trials:
                for n in range(N):
                    model_ll[b_idx, s_idx, x_idx] += hmm_normalizer(log_pi0 = normalized_log_pi0, 
                                                                    log_Ps = normalized_log_trans_matrix, 
                                                                    ll = lls[n])

    # convert all nan to -inf
    model_ll = np.nan_to_num(model_ll, nan=-np.inf)

    # Model log-posterior
    unnormalised_log_poste = model_ll + log_prior
    # print(unnormalised_log_poste)

    ramp_MLL = logsumexp_scipy(unnormalised_log_poste)
    # log_poste = unnormalised_log_poste - ramp_MLL
    # poste = np.exp(log_poste)+1e-16

    if Print:
        end_time = time.time()
        elapsed_time = end_time - start_time
        print("Ramp marginal log-likelihood inferred：", elapsed_time, "s")     
    
    start_time = time.time()
    
    # Step MLL
    log_prior = np.log(1/M**3)
    model_ll = np.zeros((M,M,M))
    for m_idx in range(M):
        for r_idx in range(M):
            for x_idx in range(M):
                m = values_m[m_idx]
                r = values_r[r_idx]
                x0 = values_x0[x_idx]

                step = StepModel(r=r, m=m, x0=x0, Rh=Rh)
                [_, _, _, normalized_log_trans_matrix, normalized_log_pi0] = step.simulate_HMM_inhomo(Ntrials=0, 
                                                                                                      T=T, 
                                                                                                      get_rate=True)

                rt = np.array([x0, 1]) * Rh * dt
                lls = poisson_logpdf(counts=counts_matrix,lambdas= rt, mask=None) # N x T x K 

                if np.isnan(lls).any():
                    print(lls)

                # Model log-likelihood for N trials:
                for n in range(N):
                    model_ll[m_idx, r_idx, x_idx] += hmm_normalizer(log_pi0 = normalized_log_pi0, 
                                                                    log_Ps = normalized_log_trans_matrix, 
                                                                    ll = lls[n])
                if np.isnan(model_ll[m_idx, r_idx, x_idx]):
                    print(normalized_log_trans_matrix)
                    print(lls)

    # convert all nan to -inf
    model_ll = np.nan_to_num(model_ll, nan=-np.inf)

    # Model log-posterior
    unnormalised_log_poste = model_ll + log_prior
    # print(unnormalised_log_poste)

    step_MLL = logsumexp_scipy(unnormalised_log_poste)
    # log_poste = unnormalised_log_poste - step_MLL
    # poste = np.exp(log_poste) + 1e-16
    if Print:
        end_time = time.time()
        elapsed_time = end_time - start_time
        print("Step marginal log-likelihood inferred：", elapsed_time, "s") 


    logMLR = ramp_MLL - step_MLL
    return logMLR, ramp_MLL, step_MLL


def classifier_tester(M=100, N=400, T=100, classifier="mlr", thresholds=None):
    start_time = time.time()

    data_points = generate_test_spike_trains(M=M, N=N, T=T, model="original")
    # r, b, s are controled by exponent
    # m, x are generated linearily
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"2x{M} datasets generated (NxT = {N}x{T})：", elapsed_time, "s")     
    start_time = time.time()


    if classifier == "var":
        predictions, _, _= var_classifier(data_points, thresholds)
    if classifier == "mlr":
        predictions, _ = MLR_classifier(data_points)
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Pridictions generated：", elapsed_time, "s")     

    accuracy = (np.sum(1-predictions[0]) + np.sum(predictions[1]))/(2*M)
    return accuracy

def generate_test_spike_trains(num_grid=10, M=20, N=400, T=100, rmin=1, rmax=100, bmin=0, bmax=4, logsmin=0.04, logsmax=4, mmin=0, mmax=100, xmin=0, xmax=1, model="original", GammaShape=None):
    """
    Generate M data points for both ramp model and step model
    :param M: number of data points for each model
    :param N: number of trls per data point
    :param T: duration of each trial in number of time-steps
    :param m: mean jump time (in # of time-steps) for StepModel
    :param r: parameter r ("# of successes") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel
    :param sigma: diffusion strength of the drift-diffusion process for RampModel
    :param beta: drift rate of the drift-diffusion process for RampModel
    :return: A matrix with dim (2, M, N, T)
    """
    
       
    # Generate M data points
    data_points = np.empty((2, M, N, T))  # for an n x m array
    for MM in range(M):
  # Initialize random model parameters
        m = npr.uniform(mmin, mmax)
        r = npr.uniform(rmin, rmax)
        b = npr.uniform(bmin,bmax)
        s = np.exp(npr.uniform(logsmin,logsmax))
        xr = npr.uniform(xmin,xmax)
        xs = npr.uniform(xmin,xmax)
        
        
        #initialise models
        step_model = StepModel(m=m, r=r, x0=xs, Rh=50);
        ramp_model = RampModel(beta=b, sigma=s, x0=xr, Rh=50);

        # Generate spike trains
        if model == "original":
            step_spikes, _, _ = step_model.simulate(Ntrials=N, T=T, GammaShape = GammaShape)
            ramp_spikes, _, _ = ramp_model.simulate(Ntrials=N, T=T, GammaShape = GammaShape)
        elif model == "hmm":
            # To be added here
            step_spikes, _, _,_ ,_= step_model.simulate_HMM_inhomo(Ntrials=N, T=T, GammaShape = GammaShape)
            ramp_spikes, _, _,_,_ = ramp_model.simulate_HMM(Ntrials=N, T=T, K=100, GammaShape = GammaShape)

        # Add spike trains to data points
        data_points[0,MM]=step_spikes
        data_points[1,MM]=ramp_spikes

  
    # Convert data_points to integer type to save memory
    data_points = data_points.astype(int)
    return data_points

def generate_spike_trains(M=20, N=400, T=100, m=50, r=10, sigma=0.2, beta=0.5):
    """
    Generate M data points for both ramp model and step model
    :param M: number of data points for each model
    :param N: number of trls per data point
    :param T: duration of each trial in number of time-steps
    :param m: mean jump time (in # of time-steps) for StepModel
    :param r: parameter r ("# of successes") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel
    :param sigma: diffusion strength of the drift-diffusion process for RampModel
    :param beta: drift rate of the drift-diffusion process for RampModel
    :return: A matrix with dim (2, M, N, T)
    """
    # Initialize models
    step_model = StepModel(m=m, r=r, x0=0.2, Rh=50);
    ramp_model = RampModel(beta=beta, sigma=sigma);

    # Generate M data points
    data_points = np.empty((2, M, N, T))  # for an n x m array
    for MM in range(M):
        # Generate spike trains
        step_spikes, _, _ = step_model.simulate(Ntrials=N, T=T)
        ramp_spikes, _, _ = ramp_model.simulate(Ntrials=N, T=T)

        # Add spike trains to data points
        data_points[0,MM]=step_spikes
        data_points[1,MM]=ramp_spikes


    # Convert data_points to integer type to save memory
    data_points = data_points.astype(int)

    return data_points


def generate_raster_and_timestamps(spike_trains, plot=False):
    """
    Generate a raster plot and timestamps of the given spike trains.
    :param spike_trains: spike trains to plot (N by T matrix)
    :param plot: whether to plot the raster
    :return: spike trains timestamps
    """
    
    
    T = len(spike_trains[0])
    # Record time of spikes in milliseconds
    spike_trains_timestamp = []
    for spike_train in spike_trains:  # for each trial
        timestamp = []
#         print(spike_train)
        for ii in range(len(spike_train)):  # for each time point
#             print(spike_train[ii])
            for jj in range(spike_train[ii]):  # handle multiple spikes in a time stamp
                timestamp.append(ii*1e3/T)
        spike_trains_timestamp.append(timestamp)

    if plot:
        fig, ax = plt.subplots()
        fig.suptitle("Spike Raster Plot")
        colors = ['C{}'.format(i) for i in range(len(spike_trains))]  # different color for each set of neural data
        ax.eventplot(spike_trains_timestamp, colors=colors, linelengths=0.2)
        ax.yaxis.set_tick_params(labelleft=False)
        ax.set_xlabel("time from motion onset (ms)")
        ax.set_ylabel("spike trains")
        plt.show()

    return spike_trains_timestamp


def generate_psth(spike_trains, bin_size=20, bin_size_2=50, plot=False, return_counts=False):
    """
    Generate a Peri-Stimulus Time Histogram (PSTH) from given timestamps.
    :param spike_trains: spike trains to plot (N by T matrix)
    :param bin_size: bin size for the PSTH (in milliseconds)
    :param bin_size2: a larger size to calculate the variance of psth (in milliseconds)
    :param plot: whether to plot the PSTH
    :return: averaged PSTH, smoothed_psth, variance, Fano factor
    """
    
    N = spike_trains.shape[0]; # number of trials
    T = spike_trains.shape[1];
    #print(N)
    spike_trains_timestamp = generate_raster_and_timestamps(spike_trains); # timestamps of spike trains
    
    
    if return_counts == True:
        counts_matrix = np.zeros((N, T)); # (N x T）
        # Calculate the PSTH for each trail
        for ii in range(len(spike_trains_timestamp)):
            bin_edges_for_counts = np.arange(0, 1e3 + 1000/T, 1000/T)
            counts_matrix[ii], _ = np.histogram(np.array(spike_trains_timestamp[ii]), bins=bin_edges_for_counts)
        counts_matrix = counts_matrix.astype(int)
        return counts_matrix
    
    
    # Calculate the PSTH
    bin_edges = np.arange(0, 1e3 + bin_size, bin_size)
    psth, _ = np.histogram(np.concatenate(spike_trains_timestamp), bins=bin_edges)

    averaged_psth = (psth / bin_size * 1e3) / N # spikes per sec per trail

    # Apply Gaussian smoothing
    sigma = 1.5  # Standard deviation of the Gaussian filter
    gaussian_smoothed_psth = gaussian_filter(averaged_psth, sigma)

    # Calculate the PSTH for larger bins
    bin_edges_2 = np.arange(0, 1e3+bin_size_2, bin_size_2)
    psth_2, _ = np.histogram(np.concatenate(spike_trains_timestamp), bins=bin_edges_2)
    averaged_psth_2 = psth_2 / N # spikes per trail

    var_s = np.zeros_like(averaged_psth_2)

    # psth_matrix is a 2D numpy array where each row is a PSTH vector
    psth_matrix = np.zeros((N, len(averaged_psth_2))); # (N x time_bins)

    # Calculate the PSTH for each trail
    for ii in range(len(spike_trains_timestamp)):
        psth_matrix[ii], _ = np.histogram(np.array(spike_trains_timestamp[ii]), bins=bin_edges_2)
    
    
#     print(psth_matrix.shape)
    # Find the variance across trials (i.e., along the rows)
    var_s = np.var(psth_matrix, axis=0);

    ## Calculate Fano Factor ##

    fano_factors = var_s / averaged_psth_2


    if plot:
        fig, (ax1, ax2, ax3) = plt.subplots(3)
        fig.suptitle("PSTH Diagram")

        # Plot the PSTH
        ax1.plot(bin_edges[:-1], averaged_psth,  label='Original')
        ax1.plot(bin_edges[:-1], gaussian_smoothed_psth,  label='Smoothed')
        ax2.plot(bin_edges_2[:-1], var_s,  label='Variance')
        ax3.plot(bin_edges_2[:-1], fano_factors,  label='Fano factor')

        ax1.set_ylabel("spike rate (sp/s)")
        ax2.set_ylabel("Variance")
        ax3.set_ylabel("Fano factor")
        ax3.set_xlabel("time from motion onset (ms)")
        ax1.legend()
        ax2.legend()
        plt.show()
        
    return averaged_psth, gaussian_smoothed_psth, var_s, fano_factors




def var_classifier(data_points, thresholds):
    """
    Classify spike trains as being generated by the step model (return 0) or the ramp model (return 1).
    :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix
    :param m: mean jump time (in # of time-steps) for StepModel
    :param r: parameter r ("# of successes") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel
    :param sigma: diffusion strength of the drift-diffusion process for RampModel
    :param beta: drift rate of the drift-diffusion process for RampModel
    :param threshold: threshold for variance
    :return: M predictions, each being 0 (step model) or 1 (ramp model)
    """
    predictions = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
    var_s = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
    fano_factors = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
    
    for ii in [0,1]:
        # ii = 0 -> STEP spike trains
        # ii = 1 -> RAMP spike trains
        for jj in range(data_points[ii].shape[0]): 
            spike_trains = data_points[ii, jj]; # (N by T) spike train matrix
            # Calculate the PSTH
            _,psth,_,_ = generate_psth(spike_trains, bin_size=20, bin_size_2=50)

            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            # Scale the PSTH 
    #         psth_step_scaled = psth_step * 2 * m / len(psth_step)
    #         psth_ramp_scaled = psth_ramp * 2 * m / len(psth_ramp)

            # Find the gradient of the PSTH
            grad_psth = np.gradient(psth)

            # Find the variance and the Fano factor of the gradient
            var = np.var(grad_psth)
            fano_factor = var / np.mean(grad_psth)
            
            # Print the variance and the Fano factor
            # print(f"variance = {var}, Fano factor = {fano_factor}")
            var_s[ii,jj] = var
            fano_factors[ii,jj] = fano_factor

            # Classify the spike trains based on the variance 
            if var > thresholds:
                predictions[ii, jj] = 0  # step model
            else:
                predictions[ii, jj] = 1  # ramp model

#             if var_ramp > threshold:
#                 predictions.append(0)  # step model
#             else:
#                 predictions.append(1)  # ramp model

    return predictions, var_s, fano_factors

def higher_order_classifier(data_points, thresholds):
    """
    Classify spike trains as being generated by the step model (return 0) or the ramp model (return 1).
    :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix
    :param m: mean jump time (in # of time-steps) for StepModel
    :param r: parameter r ("# of successes") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel
    :param sigma: diffusion strength of the drift-diffusion process for RampModel
    :param beta: drift rate of the drift-diffusion process for RampModel
    :param threshold: threshold for variance
    :return: M predictions, each being 0 (step model) or 1 (ramp model)
    """
    predictions = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
    var_s = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
    fano_factors = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
    
    for ii in [0,1]:
        # ii = 0 -> STEP spike trains
        # ii = 1 -> RAMP spike trains
        for jj in range(data_points[ii].shape[0]): 
            spike_trains = data_points[ii, jj]; # (N by T) spike train matrix
            # Calculate the PSTH
            _,psth,_,_ = generate_psth(spike_trains, bin_size=20, bin_size_2=50)
            
            
            ### find the vilid region
            
            
            
            ### Gradient and average gradient

            # Find the gradient of the PSTH
            grad_psth = np.gradient(psth)
            average_grad = (psth[-1] - psth[1]) / len(psth)
            # Find the variance and the Fano factor of the gradient
            var = np.var(grad_psth)
            fano_factor = var / np.mean(grad_psth)
            
            # Print the variance and the Fano factor
            # print(f"variance = {var}, Fano factor = {fano_factor}")
            var_s[ii,jj] = var
            fano_factors[ii,jj] = fano_factor

            # Classify the spike trains based on the variance 
            if var > thresholds:
                predictions[ii, jj] = 0  # step model
            else:
                predictions[ii, jj] = 1  # ramp model

#             if var_ramp > threshold:
#                 predictions.append(0)  # step model
#             else:
#                 predictions.append(1)  # ramp model

    return predictions, var_s, fano_factors


       

# def normalised_var_classifier(data_points, m, r, sigma, beta, threshold):
#     """
#     Classify spike trains as being generated by the step model (return 0) or the ramp model (return 1).
#     :param data_points: 2*M data points, M data points for each model, where each data point is a (N by T) matrix
#     :param m: mean jump time (in # of time-steps) for StepModel
#     :param r: parameter r ("# of successes") of the Negative Binomial (NB) distribution of jump (stepping) time for StepModel
#     :param sigma: diffusion strength of the drift-diffusion process for RampModel
#     :param beta: drift rate of the drift-diffusion process for RampModel
#     :param threshold: threshold for variance
#     :return: M predictions, each being 0 (step model) or 1 (ramp model)
#     """
#     predictions = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
#     var_s = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
#     fano_factors = np.empty((data_points.shape[0], data_points.shape[1])) # 2 x M
    
#     for ii in [0,1]:
#         # ii = 0 -> STEP spike trains
#         # ii = 1 -> RAMP spike trains
#         for jj in range(data_points[ii].shape[0]): 
#             spike_trains = data_points[ii, jj]; # (N by T) spike train matrix
#             # Calculate the PSTH
#             psth,_,_ = generate_psth(spike_trains, bin_size=20, bin_size_2=50)

#             #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#             # Scale the PSTH 
#     #         psth_step_scaled = psth_step * 2 * m / len(psth_step)
#     #         psth_ramp_scaled = psth_ramp * 2 * m / len(psth_ramp)

#             # Find the gradient of the PSTH
#             grad_psth = np.gradient(psth)
            
#             # Normalize the gradient so that the area under it is equal to 1
#             grad_psth_normalized = grad_psth / np.sum(grad_psth)

#             # Find the variance and the Fano factor of the gradient
#             var = np.var(grad_psth_normalized)
#             fano_factor = var / np.mean(grad_psth_normalized)
            
            
#             # Print the variance and the Fano factor
#             # print(f"variance = {var}, Fano factor = {fano_factor}")
#             var_s[ii,jj] = var
#             fano_factors[ii,jj] = fano_factor
#             #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#             # Classify the spike trains based on the variance 
# #             if var > threshold:
# #                 predictions[ii,].append(0)  # step model
# #             else:
# #                 predictions[ii].append(1)  # ramp model

# #             if var_ramp > threshold:
# #                 predictions.append(0)  # step model
# #             else:
# #                 predictions.append(1)  # ramp model

#     return predictions, var_s, fano_factors
